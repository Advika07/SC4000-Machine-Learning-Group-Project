{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ca37ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9043a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "CFG = {\n",
    "\n",
    "    # official data paths\n",
    "    \"train_csv\": \"./data/train.csv\",\n",
    "    \"test_csv\": \"./data/test.csv\",\n",
    "\n",
    "    # Teacher outputs (A/B/Tie) on TRAIN ONLY\n",
    "    # Expected file: submission.csv under the runs folder\n",
    "    #   columns: id + either\n",
    "    #     (pA, pB, pTie)  OR  (logit_A, logit_B, logit_Tie)\n",
    "    \"teacher_train_path\": \"./runs/submission.csv\",\n",
    "\n",
    "    # Where to save student artifacts (used by combination.ipynb)\n",
    "    \"out_dir\": \"./student_outputs\",\n",
    "    \"oof_path\": \"./student_outputs/student_oof_probs.csv\",\n",
    "    \"test_pred_path\": \"./student_outputs/student_test_probs.csv\",\n",
    "    \"avg_model_path\": \"./student_outputs/student_model_avg.pt\",\n",
    "\n",
    "    # Base model (student backbone)\n",
    "    # You can change this to a smaller model if you can't run Gemma-2-9B.\n",
    "    \"model_name\": \"microsoft/deberta-v3-base\",  # e.g. \"microsoft/deberta-v3-base\"\n",
    "    \"max_length\": 512,\n",
    "    \"num_labels\": 3,  # classes: A, B, Tie\n",
    "\n",
    "    # LoRA hyperparams\n",
    "    \"lora_r\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "\n",
    "    # Training hyperparams\n",
    "    \"n_folds\": 5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"train_batch_size\": 2,\n",
    "    \"eval_batch_size\": 4,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "\n",
    "    # Knowledge distillation hyperparams\n",
    "    \"use_kd\": True,\n",
    "    \"temperature\": 2.0,   # T in KL term\n",
    "    \"ce_weight\": 1.0,     # weight on CE(y, p_student)\n",
    "    \"kl_weight\": 1.0,     # weight on KL(teacher_T || student)\n",
    "\n",
    "    # Misc\n",
    "    \"seed\": 42,\n",
    "    \"num_workers\": 0,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc1ac7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "DEVICE = get_device()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa8f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# UTILS\n",
    "# ==========================\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    \"\"\"Create directory if it does not exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def label_from_winners(row: pd.Series) -> int:\n",
    "    \"\"\"\n",
    "    Map competition winners to integer labels:\n",
    "      0 = A wins, 1 = B wins, 2 = Tie\n",
    "    Assumes train.csv has columns: winner_model_a, winner_model_b, winner_tie.\n",
    "    \"\"\"\n",
    "    if row[\"winner_model_a\"] == 1:\n",
    "        return 0\n",
    "    if row[\"winner_model_b\"] == 1:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "\n",
    "def make_prompt_groups(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    GroupKFold groups by prompt so the same prompt never appears in both\n",
    "    train and validation for a given fold.\n",
    "    \"\"\"\n",
    "    return df[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9)).values\n",
    "\n",
    "\n",
    "def load_official_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load train/test CSVs and derive integer labels for train.\n",
    "    \"\"\"\n",
    "    train = pd.read_csv(CFG[\"train_csv\"])\n",
    "    test = pd.read_csv(CFG[\"test_csv\"])\n",
    "\n",
    "    # derive labels 0/1/2 from winner columns\n",
    "    train[\"label\"] = train.apply(label_from_winners, axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def load_teacher_train_logits(train_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load teacher outputs for TRAIN rows, aligned by id.\n",
    "\n",
    "    teacher_logits_train.csv must contain:\n",
    "      - 'id'\n",
    "      - either (pA, pB, pTie)  -> probabilities\n",
    "        OR    (logit_A, logit_B, logit_Tie) -> raw logits\n",
    "\n",
    "    Returns: numpy array of shape [N_train, 3] with logits for A/B/Tie.\n",
    "    \"\"\"\n",
    "    path = CFG[\"teacher_train_path\"]\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Teacher train file not found at {path}. \"\n",
    "            f\"Make sure teacher_logits_train.csv exists.\"\n",
    "        )\n",
    "\n",
    "    tdf = pd.read_csv(path)\n",
    "    if \"id\" not in tdf.columns:\n",
    "        raise ValueError(\"Teacher file must contain an 'id' column.\")\n",
    "\n",
    "    tdf = tdf.set_index(\"id\")\n",
    "\n",
    "    # Check all train ids are present in teacher file\n",
    "    if not train_df[\"id\"].isin(tdf.index).all():\n",
    "        missing = train_df.loc[~train_df[\"id\"].isin(tdf.index), \"id\"]\n",
    "        raise ValueError(\n",
    "            f\"Teacher file is missing {len(missing)} train ids, e.g. {missing.iloc[:5].tolist()}\"\n",
    "        )\n",
    "\n",
    "    # Case 1: teacher saved probabilities pA/pB/pTie\n",
    "    if {\"pA\", \"pB\", \"pTie\"}.issubset(tdf.columns):\n",
    "        probs = tdf.loc[train_df[\"id\"], [\"pA\", \"pB\", \"pTie\"]].values.astype(np.float32)\n",
    "        eps = 1e-9\n",
    "        probs = np.clip(probs, eps, 1.0 - eps)\n",
    "        logits = np.log(probs)  # safe \"logits\" for KD\n",
    "        return logits\n",
    "\n",
    "    # Case 2: teacher saved explicit logits\n",
    "    alt_cols = [\"logit_A\", \"logit_B\", \"logit_Tie\"]\n",
    "    if set(alt_cols).issubset(tdf.columns):\n",
    "        logits = tdf.loc[train_df[\"id\"], alt_cols].values.astype(np.float32)\n",
    "        return logits\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Teacher file does not have expected columns. \"\n",
    "        \"Need either [pA, pB, pTie] or [logit_A, logit_B, logit_Tie].\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34513939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# DATASET\n",
    "# ==========================\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Torch Dataset for one row:\n",
    "      (prompt, response_a, response_b, [label], [teacher_logits])\n",
    "\n",
    "    label:       0/1/2 (A/B/Tie), for train/val\n",
    "    teacher_logits: optional KD target, shape [3]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        max_length: int,\n",
    "        teacher_logits: Optional[np.ndarray] = None,\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if teacher_logits is not None:\n",
    "            assert len(teacher_logits) == len(self.df)\n",
    "            self.teacher_logits = torch.tensor(teacher_logits, dtype=torch.float32)\n",
    "        else:\n",
    "            self.teacher_logits = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        prompt = str(row[\"prompt\"])\n",
    "        ra = str(row[\"response_a\"])\n",
    "        rb = str(row[\"response_b\"])\n",
    "\n",
    "        # Simple text framing for the student\n",
    "        text = (\n",
    "            \"Prompt:\\n\"\n",
    "            + prompt\n",
    "            + \"\\n\\nResponse A:\\n\"\n",
    "            + ra\n",
    "            + \"\\n\\nResponse B:\\n\"\n",
    "            + rb\n",
    "        )\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "        # label only for train/val\n",
    "        if \"label\" in self.df.columns:\n",
    "            item[\"labels\"] = int(row[\"label\"])\n",
    "\n",
    "        # KD targets if provided\n",
    "        if self.teacher_logits is not None:\n",
    "            item[\"teacher_logits\"] = self.teacher_logits[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate dicts from Dataset into batch tensors.\n",
    "    Handles presence/absence of labels and teacher_logits.\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([x[\"input_ids\"] for x in batch])\n",
    "    attention_mask = torch.stack([x[\"attention_mask\"] for x in batch])\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "    if \"labels\" in batch[0]:\n",
    "        labels = torch.tensor([x[\"labels\"] for x in batch], dtype=torch.long)\n",
    "        result[\"labels\"] = labels\n",
    "\n",
    "    if \"teacher_logits\" in batch[0]:\n",
    "        tlogits = torch.stack([x[\"teacher_logits\"] for x in batch])\n",
    "        result[\"teacher_logits\"] = tlogits\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83bf994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# MODEL + LORA HELPERS\n",
    "# ==========================\n",
    "def build_tokenizer_and_model():\n",
    "    \"\"\"\n",
    "    Build tokenizer + base sequence-classification model and wrap with LoRA.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG[\"model_name\"], use_fast=False)\n",
    "    # Some models (e.g. Gemma) may not have pad_token; reuse eos_token.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CFG[\"model_name\"],\n",
    "        num_labels=CFG[\"num_labels\"],\n",
    "    )\n",
    "\n",
    "    # Automatically find all linear modules to attach LoRA to\n",
    "    def find_all_linear_names(model):\n",
    "        cls = torch.nn.Linear\n",
    "        lora_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                parts = name.split(\".\")\n",
    "                lora_module_names.add(parts[-1])\n",
    "        # Usually we do NOT LoRA the final classifier head\n",
    "        for bad in [\"classifier\", \"score\"]:\n",
    "            if bad in lora_module_names:\n",
    "                lora_module_names.remove(bad)\n",
    "        return list(lora_module_names)\n",
    "\n",
    "    target_modules = find_all_linear_names(base_model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=CFG[\"lora_r\"],\n",
    "        lora_alpha=CFG[\"lora_alpha\"],\n",
    "        lora_dropout=CFG[\"lora_dropout\"],\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def kd_loss_fn(\n",
    "    student_logits: torch.Tensor,\n",
    "    teacher_logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    L = ce_weight * CE(y, p_student) + kl_weight * KL(softmax(teacher/T) || p_student)\n",
    "\n",
    "    - CE term uses hard labels (competition labels).\n",
    "    - KL term encourages student to mimic teacher distribution (softer signal).\n",
    "    \"\"\"\n",
    "    T = CFG[\"temperature\"]\n",
    "    ce_w = CFG[\"ce_weight\"]\n",
    "    kl_w = CFG[\"kl_weight\"]\n",
    "\n",
    "    # standard cross-entropy\n",
    "    ce = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "    # teacher soft probabilities at temperature T (no grad)\n",
    "    with torch.no_grad():\n",
    "        t_probs_T = F.softmax(teacher_logits / T, dim=-1)\n",
    "\n",
    "    # KL(p_teacher_T || p_student)\n",
    "    log_p_student = F.log_softmax(student_logits, dim=-1)\n",
    "    kl = F.kl_div(\n",
    "        log_p_student,\n",
    "        t_probs_T,\n",
    "        reduction=\"batchmean\",\n",
    "    )\n",
    "\n",
    "    return ce_w * ce + kl_w * kl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9541bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# TRAIN / EVAL LOOPS\n",
    "# ==========================\n",
    "def train_one_fold(\n",
    "    fold: int,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    teacher_logits_train: np.ndarray,\n",
    "    train_idx: np.ndarray,\n",
    "    val_idx: np.ndarray,\n",
    ") -> Tuple[np.ndarray, Dict, dict]:\n",
    "    \"\"\"\n",
    "    Train on a single fold and return:\n",
    "      - val_probs: (len(val_df), 3) predicted probabilities\n",
    "      - fold_metrics: dict with log-loss, etc.\n",
    "      - state_dict: model.state_dict() snapshot for this fold\n",
    "    \"\"\"\n",
    "    device = CFG[\"device\"]\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Teacher logits for the train fold (KD on train only)\n",
    "    fold_teacher_logits = teacher_logits_train[train_idx] if CFG[\"use_kd\"] else None\n",
    "\n",
    "    train_dataset = PreferenceDataset(\n",
    "        train_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=CFG[\"max_length\"],\n",
    "        teacher_logits=fold_teacher_logits,\n",
    "    )\n",
    "    val_dataset = PreferenceDataset(\n",
    "        val_df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=CFG[\"max_length\"],\n",
    "        teacher_logits=None,  # we don't need KD on val\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CFG[\"train_batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=CFG[\"num_workers\"],\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CFG[\"eval_batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=CFG[\"num_workers\"],\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    # Optimizer with weight decay on non-bias/non-LayerNorm params\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": CFG[\"weight_decay\"],\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=CFG[\"learning_rate\"],\n",
    "    )\n",
    "\n",
    "    # Cosine schedule with warmup\n",
    "    num_training_steps = CFG[\"num_epochs\"] * len(train_loader)\n",
    "    num_warmup_steps = int(CFG[\"warmup_ratio\"] * num_training_steps)\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(CFG[\"num_epochs\"]):\n",
    "        pbar = tqdm(train_loader, desc=f\"Fold {fold} | Epoch {epoch+1}/{CFG['num_epochs']}\")\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            student_logits = outputs.logits\n",
    "\n",
    "            if CFG[\"use_kd\"] and \"teacher_logits\" in batch:\n",
    "                t_logits = batch[\"teacher_logits\"].to(device)\n",
    "                loss = kd_loss_fn(student_logits, t_logits, labels)\n",
    "            else:\n",
    "                loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Evaluation on validation split\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validating Fold {fold}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    fold_ll = log_loss(all_labels, all_probs, labels=[0, 1, 2])\n",
    "\n",
    "    fold_metrics = {\"fold\": fold, \"log_loss\": float(fold_ll), \"n_val\": int(len(val_df))}\n",
    "    state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    return all_probs, fold_metrics, state_dict\n",
    "\n",
    "\n",
    "def average_state_dicts(state_dicts: List[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Simple element-wise average of a list of state_dicts.\n",
    "    All models must share the same architecture.\n",
    "    \"\"\"\n",
    "    avg_state = {}\n",
    "    n = len(state_dicts)\n",
    "    keys = state_dicts[0].keys()\n",
    "    for k in keys:\n",
    "        avg_state[k] = sum(sd[k] for sd in state_dicts) / n\n",
    "    return avg_state\n",
    "\n",
    "\n",
    "def predict_with_model(model, tokenizer, df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run a trained model on df and return probabilities (N, 3).\n",
    "    \"\"\"\n",
    "    device = CFG[\"device\"]\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataset = PreferenceDataset(\n",
    "        df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=CFG[\"max_length\"],\n",
    "        teacher_logits=None,\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG[\"eval_batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=CFG[\"num_workers\"],\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    probs_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            probs_all.append(probs)\n",
    "\n",
    "    probs_all = np.concatenate(probs_all, axis=0)\n",
    "    assert len(probs_all) == len(df)\n",
    "    return probs_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd120de",
   "metadata": {},
   "source": [
    "new set of code with checkpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfc65155",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG[\"ckpt_dir\"] = \"./student_checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240cc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    CFG[\"device\"] = get_device()\n",
    "    seed_everything(CFG[\"seed\"])\n",
    "    ensure_dir(CFG[\"out_dir\"])\n",
    "    ensure_dir(CFG[\"ckpt_dir\"])\n",
    "\n",
    "    print(\"Loading official train/test...\")\n",
    "    train_df, test_df = load_official_data()\n",
    "\n",
    "    print(\"Loading teacher logits for KD...\")\n",
    "    teacher_logits_train = load_teacher_train_logits(train_df)\n",
    "\n",
    "    print(\"Building tokenizer (once) to use across folds...\")\n",
    "    tokenizer, _ = build_tokenizer_and_model()\n",
    "\n",
    "    groups = make_prompt_groups(train_df)\n",
    "    gkf = GroupKFold(n_splits=CFG[\"n_folds\"])\n",
    "\n",
    "    oof_preds = np.zeros((len(train_df), CFG[\"num_labels\"]), dtype=np.float32)\n",
    "    fold_metrics_list, fold_state_dicts = [], []\n",
    "\n",
    "    # ----- LOOP OVER FOLDS -----\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(train_df, groups=groups)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{CFG['n_folds']} ===\")\n",
    "        fold_final = os.path.join(CFG[\"ckpt_dir\"], f\"fold{fold}_final.pt\")\n",
    "        fold_oof = os.path.join(CFG[\"ckpt_dir\"], f\"fold{fold}_oof.csv\")\n",
    "\n",
    "        train_sub = train_df.iloc[tr_idx].reset_index(drop=True)\n",
    "        val_sub = train_df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "        # if fold already trained -> skip\n",
    "        if os.path.exists(fold_final) and os.path.exists(fold_oof):\n",
    "            print(f\"[Fold {fold}] Found finished fold, skipping.\")\n",
    "            fold_state_dicts.append(torch.load(fold_final, map_location=\"cpu\"))\n",
    "            oof_preds[va_idx] = pd.read_csv(fold_oof)[[\"pA\", \"pB\", \"pTie\"]].values\n",
    "            fold_ll = log_loss(val_sub[\"label\"], oof_preds[va_idx], labels=[0, 1, 2])\n",
    "            fold_metrics_list.append({\"fold\": fold, \"log_loss\": fold_ll})\n",
    "            continue\n",
    "\n",
    "        # if partial checkpoints exist -> resume from last epoch\n",
    "        resume_ckpt = None\n",
    "        ep_files = [f for f in os.listdir(CFG[\"ckpt_dir\"]) if f.startswith(f\"fold{fold}_epoch\")]\n",
    "        if ep_files:\n",
    "            ep_files.sort(key=lambda x: int(x.split(\"epoch\")[-1].split(\".\")[0]))\n",
    "            resume_ckpt = os.path.join(CFG[\"ckpt_dir\"], ep_files[-1])\n",
    "            print(f\"[Fold {fold}] Resuming from {resume_ckpt}\")\n",
    "\n",
    "        # new model each fold\n",
    "        _, model = build_tokenizer_and_model()\n",
    "\n",
    "        val_probs, fold_metrics, state_dict = train_one_fold(\n",
    "            fold=fold,\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_df=train_sub,\n",
    "            val_df=val_sub,\n",
    "            teacher_logits_train=teacher_logits_train,\n",
    "            train_idx=tr_idx,\n",
    "            val_idx=va_idx,\n",
    "        )\n",
    "\n",
    "        # save OOF + final weights\n",
    "        pd.DataFrame({\n",
    "            \"id\": val_sub[\"id\"],\n",
    "            \"pA\": val_probs[:, 0],\n",
    "            \"pB\": val_probs[:, 1],\n",
    "            \"pTie\": val_probs[:, 2],\n",
    "        }).to_csv(fold_oof, index=False)\n",
    "        torch.save(state_dict, fold_final)\n",
    "\n",
    "        oof_preds[va_idx] = val_probs\n",
    "        fold_metrics_list.append(fold_metrics)\n",
    "        fold_state_dicts.append(state_dict)\n",
    "        print(f\"Fold {fold}: log-loss = {fold_metrics['log_loss']:.6f}\")\n",
    "\n",
    "    # ----- AFTER ALL FOLDS -----\n",
    "    overall_ll = log_loss(train_df[\"label\"], oof_preds, labels=[0, 1, 2])\n",
    "    print(f\"\\nOverall OOF log-loss: {overall_ll:.6f}\")\n",
    "\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": train_df[\"id\"],\n",
    "        \"pA\": oof_preds[:, 0],\n",
    "        \"pB\": oof_preds[:, 1],\n",
    "        \"pTie\": oof_preds[:, 2],\n",
    "    })\n",
    "    oof_df.to_csv(CFG[\"oof_path\"], index=False)\n",
    "    print(f\"Saved OOF -> {CFG['oof_path']}\")\n",
    "\n",
    "    print(\"Averaging model weights...\")\n",
    "    avg_state = average_state_dicts(fold_state_dicts)\n",
    "    _, final_model = build_tokenizer_and_model()\n",
    "    final_model.load_state_dict(avg_state)\n",
    "    torch.save(avg_state, CFG[\"avg_model_path\"])\n",
    "    print(f\"Saved averaged weights -> {CFG['avg_model_path']}\")\n",
    "\n",
    "    print(\"Predicting on test set...\")\n",
    "    test_probs = predict_with_model(final_model, tokenizer, test_df)\n",
    "    pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"pA\": test_probs[:, 0],\n",
    "        \"pB\": test_probs[:, 1],\n",
    "        \"pTie\": test_probs[:, 2],\n",
    "    }).to_csv(CFG[\"test_pred_path\"], index=False)\n",
    "    print(f\"Saved test predictions -> {CFG['test_pred_path']}\")\n",
    "\n",
    "    # summary\n",
    "    metrics = {\"folds\": fold_metrics_list, \"overall_oof_log_loss\": float(overall_ll)}\n",
    "    json.dump(metrics, open(os.path.join(CFG[\"out_dir\"], \"student_training_report.json\"), \"w\"), indent=2)\n",
    "    print(\"✅ Training complete — checkpoints are saved, you can resume anytime!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading official train/test...\n",
      "Loading teacher logits for KD...\n",
      "Building tokenizer (once) to use across folds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,717,443 || all params: 195,141,894 || trainable%: 5.4921\n",
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,717,443 || all params: 195,141,894 || trainable%: 5.4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 0 | Epoch 1/3: 100%|██████████| 22991/22991 [2:04:37<00:00,  3.07it/s, loss=1.2328]  \n",
      "Fold 0 | Epoch 2/3: 100%|██████████| 22991/22991 [1:50:37<00:00,  3.46it/s, loss=0.9623]\n",
      "Fold 0 | Epoch 3/3: 100%|██████████| 22991/22991 [1:50:15<00:00,  3.48it/s, loss=2.2260]\n",
      "Validating Fold 0: 100%|██████████| 2874/2874 [11:11<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: log-loss = 1.029039\n",
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,717,443 || all params: 195,141,894 || trainable%: 5.4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 | Epoch 1/3: 100%|██████████| 22991/22991 [1:50:00<00:00,  3.48it/s, loss=1.2175]\n",
      "Fold 1 | Epoch 2/3: 100%|██████████| 22991/22991 [1:50:01<00:00,  3.48it/s, loss=1.2505]\n",
      "Fold 1 | Epoch 3/3: 100%|██████████| 22991/22991 [1:50:24<00:00,  3.47it/s, loss=1.1449]\n",
      "Validating Fold 1: 100%|██████████| 2874/2874 [11:19<00:00,  4.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: log-loss = 1.042005\n",
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,717,443 || all params: 195,141,894 || trainable%: 5.4921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 | Epoch 1/3: 100%|██████████| 22991/22991 [1:51:06<00:00,  3.45it/s, loss=1.0767]\n",
      "Fold 2 | Epoch 2/3: 100%|██████████| 22991/22991 [1:56:45<00:00,  3.28it/s, loss=1.1375]\n",
      "Fold 2 | Epoch 3/3: 100%|██████████| 22991/22991 [1:54:27<00:00,  3.35it/s, loss=1.1529]\n",
      "Validating Fold 2: 100%|██████████| 2874/2874 [10:59<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2: log-loss = 1.033705\n",
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
