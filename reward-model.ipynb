{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe86021",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers peft accelerate datasets tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9017233",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DATA_DIR = \"lmsys-chatbot-arena\"  # based on your screenshot\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "BASE_MODEL_NAME = \"roberta-large\"   # can downgrade to \"roberta-base\" if VRAM explodes\n",
    "MAX_LEN = 768                       # keep this reasonable for your GPU\n",
    "BATCH_SIZE = 2                      # we use grad accumulation instead of big batch\n",
    "GRAD_ACCUM_STEPS = 8                # effective batch_size ~= 16\n",
    "LR = 2e-5\n",
    "EPOCHS = 2\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "OUT_DIR = \"reward_teacher_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81176b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# sanity: make one label column 0/1/2\n",
    "def row_label(r):\n",
    "    if r[\"winner_model_a\"] == 1:\n",
    "        return 0  # A preferred\n",
    "    elif r[\"winner_model_b\"] == 1:\n",
    "        return 1  # B preferred\n",
    "    else:\n",
    "        return 2  # tie\n",
    "\n",
    "df[\"label\"] = df.apply(row_label, axis=1)\n",
    "\n",
    "# We also keep lengths if you want analysis later\n",
    "df[\"len_a\"] = df[\"response_a\"].astype(str).str.len()\n",
    "df[\"len_b\"] = df[\"response_b\"].astype(str).str.len()\n",
    "\n",
    "# groups for CV: group by prompt (hash to avoid giant strings)\n",
    "df[\"prompt_group\"] = df[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9))\n",
    "print(df[[\"label\",\"len_a\",\"len_b\"]].head())\n",
    "print(\"class counts:\\n\", df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a15403",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class PairwisePrefDataset(Dataset):\n",
    "    def __init__(self, df_fold, tokenizer, max_len=512):\n",
    "        self.df = df_fold.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def encode_pair(self, prompt, resp):\n",
    "        # We'll just concat prompt + </s> + resp style for RoBERTa:\n",
    "        text = f\"Prompt: {prompt}\\nAnswer:\\n{resp}\"\n",
    "        return self.tok(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.encode_pair(row[\"prompt\"], row[\"response_a\"])\n",
    "        enc_b = self.encode_pair(row[\"prompt\"], row[\"response_b\"])\n",
    "        label = row[\"label\"]\n",
    "        return {\n",
    "            \"input_ids_a\": enc_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": enc_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": enc_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": enc_b[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"row_id\": torch.tensor(row[\"id\"], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef49d99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class RewardScorer(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(base_model_name)\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.score_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # take CLS token embedding (RoBERTa: first token <s>)\n",
    "        cls_state = out.last_hidden_state[:, 0, :]  # [batch, hidden]\n",
    "        score = self.score_head(cls_state)          # [batch, 1]\n",
    "        return score.squeeze(-1)                    # [batch]\n",
    "\n",
    "# build model + apply LoRA\n",
    "def build_lora_reward_model(base_model_name):\n",
    "    base = RewardScorer(base_model_name)\n",
    "\n",
    "    # we apply LoRA to all linear layers in the backbone for efficiency\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"dense\", \"out_proj\"],  # broad but safe guess for RoBERTa MHA/FFN\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"  # close enough; it's scoring\n",
    "    )\n",
    "    lora_model = get_peft_model(base, peft_config)\n",
    "    return lora_model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "print(\"Model ready on\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e7101",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def pref_loss(scores_a, scores_b, labels, margin=0.5, tie_band=0.2):\n",
    "    # scores_a, scores_b: [batch]\n",
    "    # labels: [batch] in {0,1,2}\n",
    "    loss_list = []\n",
    "\n",
    "    # A wins -> sA >= sB + margin\n",
    "    mask_a = (labels == 0)\n",
    "    if mask_a.any():\n",
    "        diff = scores_b[mask_a] + margin - scores_a[mask_a]\n",
    "        # want diff <= 0\n",
    "        loss_a = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_a)\n",
    "\n",
    "    # B wins -> sB >= sA + margin\n",
    "    mask_b = (labels == 1)\n",
    "    if mask_b.any():\n",
    "        diff = scores_a[mask_b] + margin - scores_b[mask_b]\n",
    "        loss_b = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_b)\n",
    "\n",
    "    # Tie -> |sA - sB| <= tie_band\n",
    "    mask_t = (labels == 2)\n",
    "    if mask_t.any():\n",
    "        diff = torch.abs(scores_a[mask_t] - scores_b[mask_t]) - tie_band\n",
    "        loss_t = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_t)\n",
    "\n",
    "    if len(loss_list) == 0:\n",
    "        return torch.tensor(0.0, device=scores_a.device, requires_grad=True)\n",
    "    return torch.stack(loss_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31c9b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "folds = []\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(gkf.split(df, groups=df[\"prompt_group\"])):\n",
    "    df.loc[va_idx, \"fold\"] = fold_id\n",
    "df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "\n",
    "def train_one_fold(fold_id):\n",
    "    print(f\"\\n===== FOLD {fold_id} =====\")\n",
    "    df_tr = df[df[\"fold\"] != fold_id].reset_index(drop=True)\n",
    "    df_va = df[df[\"fold\"] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    train_ds = PairwisePrefDataset(df_tr, tokenizer, max_len=MAX_LEN)\n",
    "    va_ds    = PairwisePrefDataset(df_va, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_loader    = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # simple linear warmup + cosine cooldown\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS) * EPOCHS\n",
    "    warmup_steps = min(100, total_steps // 10)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(1, warmup_steps)\n",
    "        # cosine decay after warmup\n",
    "        progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold_id} Epoch {epoch}\")):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a)\n",
    "            scores_b = model(input_ids_b, attn_b)\n",
    "\n",
    "            loss = pref_loss(scores_a, scores_b, labels)\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        print(f\"Epoch {epoch} avg loss (divided): {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # after training, eval on val fold to get a feel\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_ids    = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(va_loader, desc=f\"Eval fold {fold_id}\"):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "            row_ids     = batch[\"row_id\"].cpu().numpy()\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a).cpu().numpy()\n",
    "            scores_b = model(input_ids_b, attn_b).cpu().numpy()\n",
    "\n",
    "            all_scores.append(np.stack([scores_a, scores_b], axis=1))  # shape [batch,2]\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_ids.append(row_ids)\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0)   # [N_val,2]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)   # [N_val]\n",
    "    all_ids    = np.concatenate(all_ids, axis=0)      # [N_val]\n",
    "\n",
    "    # save checkpoint\n",
    "    fold_dir = os.path.join(OUT_DIR, f\"fold_{fold_id}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(fold_dir, \"reward_teacher_roberta.pt\"))\n",
    "\n",
    "    # we also save the val fold scores for later calibration/logit building\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"score_a\": all_scores[:,0],\n",
    "        \"score_b\": all_scores[:,1],\n",
    "        \"label\": all_labels,\n",
    "        \"fold\": fold_id\n",
    "    })\n",
    "    oof_df.to_csv(os.path.join(fold_dir, \"oof_scores.csv\"), index=False)\n",
    "\n",
    "    return oof_df\n",
    "\n",
    "all_oof = []\n",
    "for f in range(N_FOLDS):\n",
    "    oof_df = train_one_fold(f)\n",
    "    all_oof.append(oof_df)\n",
    "\n",
    "all_oof = pd.concat(all_oof, ignore_index=True)\n",
    "all_oof.to_csv(os.path.join(OUT_DIR, \"all_oof_scores.csv\"), index=False)\n",
    "print(\"Saved all OOF scores to\", os.path.join(OUT_DIR, \"all_oof_scores.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbde4dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# map label int -> onehot [A,B,Tie]\n",
    "def label_to_onehot(y):\n",
    "    out = np.zeros((len(y),3), dtype=np.float32)\n",
    "    for i,lab in enumerate(y):\n",
    "        out[i, lab] = 1.0\n",
    "    return out\n",
    "\n",
    "oof = pd.read_csv(os.path.join(OUT_DIR, \"all_oof_scores.csv\"))\n",
    "\n",
    "y_true = label_to_onehot(oof[\"label\"].values)\n",
    "sa = oof[\"score_a\"].values\n",
    "sb = oof[\"score_b\"].values\n",
    "\n",
    "def probs_from_scores(sa, sb, temp, tie_alpha):\n",
    "    # Bradley-Terry-ish:\n",
    "    # pA_raw = exp(sa/temp)\n",
    "    # pB_raw = exp(sb/temp)\n",
    "    # tie_raw = exp(-abs(sa-sb)*tie_alpha)\n",
    "    pA_raw = np.exp(sa / temp)\n",
    "    pB_raw = np.exp(sb / temp)\n",
    "    tie_raw = np.exp(-np.abs(sa - sb) * tie_alpha)\n",
    "\n",
    "    denom = pA_raw + pB_raw + tie_raw + 1e-9\n",
    "    pA = pA_raw / denom\n",
    "    pB = pB_raw / denom\n",
    "    pT = tie_raw / denom\n",
    "    return np.stack([pA,pB,pT], axis=1)\n",
    "\n",
    "def logloss(params):\n",
    "    temp = np.exp(params[0])          # >0\n",
    "    tie_alpha = np.exp(params[1])     # >0\n",
    "    pred = probs_from_scores(sa, sb, temp, tie_alpha)\n",
    "    # clip for stability\n",
    "    pred = np.clip(pred, 1e-7, 1-1e-7)\n",
    "    return -np.mean((y_true * np.log(pred)).sum(axis=1))\n",
    "\n",
    "res = opt.minimize(logloss, x0=[0.0, 0.0], method=\"Nelder-Mead\")\n",
    "print(\"opt result:\", res)\n",
    "\n",
    "best_temp = math.exp(res.x[0])\n",
    "best_tie_alpha = math.exp(res.x[1])\n",
    "print(\"best_temp =\", best_temp, \" best_tie_alpha =\", best_tie_alpha)\n",
    "\n",
    "# save calibration params\n",
    "calib_path = os.path.join(OUT_DIR, \"calibration_params.json\")\n",
    "import json\n",
    "with open(calib_path, \"w\") as f:\n",
    "    json.dump({\"temp\": best_temp, \"tie_alpha\": best_tie_alpha}, f)\n",
    "print(\"Saved\", calib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede6835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"calibration_params.json\")) as f:\n",
    "    calib = json.load(f)\n",
    "TEMP = calib[\"temp\"]\n",
    "TIE_ALPHA = calib[\"tie_alpha\"]\n",
    "\n",
    "def get_model_for_fold(fold_id):\n",
    "    m = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "    state_path = os.path.join(OUT_DIR, f\"fold_{fold_id}\", \"reward_teacher_roberta.pt\")\n",
    "    m.load_state_dict(torch.load(state_path, map_location=DEVICE))\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "def score_pairs_df(df_any, max_len=MAX_LEN):\n",
    "    ds = PairwisePrefDataset(df_any, tokenizer, max_len=max_len)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    fold_scores_a = []\n",
    "    fold_scores_b = []\n",
    "    with torch.no_grad():\n",
    "        for fold_id in range(N_FOLDS):\n",
    "            model_f = get_model_for_fold(fold_id)\n",
    "            sa_list, sb_list = [], []\n",
    "            for batch in tqdm(loader, desc=f\"Infer fold {fold_id}\"):\n",
    "                input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "                attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "                input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "                attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "\n",
    "                scores_a = model_f(input_ids_a, attn_a).cpu().numpy()\n",
    "                scores_b = model_f(input_ids_b, attn_b).cpu().numpy()\n",
    "                sa_list.append(scores_a)\n",
    "                sb_list.append(scores_b)\n",
    "\n",
    "            sa_all = np.concatenate(sa_list)\n",
    "            sb_all = np.concatenate(sb_list)\n",
    "            fold_scores_a.append(sa_all)\n",
    "            fold_scores_b.append(sb_all)\n",
    "\n",
    "    # average across folds\n",
    "    sa_mean = np.mean(np.stack(fold_scores_a, axis=0), axis=0)\n",
    "    sb_mean = np.mean(np.stack(fold_scores_b, axis=0), axis=0)\n",
    "    return sa_mean, sb_mean\n",
    "\n",
    "# score train\n",
    "sa_train, sb_train = score_pairs_df(df)\n",
    "# score test\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "df_test[\"label\"] = -1  # dummy\n",
    "df_test[\"prompt_group\"] = df_test[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9))\n",
    "sa_test, sb_test = score_pairs_df(df_test)\n",
    "\n",
    "def probs_from_scores_numpy(sa, sb, temp, tie_alpha):\n",
    "    pA_raw = np.exp(sa / temp)\n",
    "    pB_raw = np.exp(sb / temp)\n",
    "    tie_raw = np.exp(-np.abs(sa - sb) * tie_alpha)\n",
    "    denom = pA_raw + pB_raw + tie_raw + 1e-9\n",
    "    pA = pA_raw / denom\n",
    "    pB = pB_raw / denom\n",
    "    pT = tie_raw / denom\n",
    "    return np.stack([pA,pB,pT], axis=1)\n",
    "\n",
    "probs_train = probs_from_scores_numpy(sa_train, sb_train, TEMP, TIE_ALPHA)\n",
    "probs_test  = probs_from_scores_numpy(sa_test,  sb_test,  TEMP, TIE_ALPHA)\n",
    "\n",
    "teacher_train_out = pd.DataFrame({\n",
    "    \"id\": df[\"id\"].values,\n",
    "    \"pA\": probs_train[:,0],\n",
    "    \"pB\": probs_train[:,1],\n",
    "    \"pTie\": probs_train[:,2],\n",
    "    \"label\": df[\"label\"].values\n",
    "})\n",
    "teacher_test_out = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"].values,\n",
    "    \"pA\": probs_test[:,0],\n",
    "    \"pB\": probs_test[:,1],\n",
    "    \"pTie\": probs_test[:,2],\n",
    "})\n",
    "\n",
    "teacher_train_out.to_csv(os.path.join(OUT_DIR, \"teacher_logits_train.csv\"), index=False)\n",
    "teacher_test_out.to_csv(os.path.join(OUT_DIR, \"teacher_logits_test.csv\"), index=False)\n",
    "\n",
    "print(\"Saved distillation targets:\")\n",
    "print(\" -\", os.path.join(OUT_DIR, \"teacher_logits_train.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"teacher_logits_test.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ece3d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
