{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe86021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp314-cp314-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Downloading triton-3.5.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.3.4-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.23-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from peft) (7.1.2)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp314-cp314-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp314-cp314-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from requests->transformers) (2.5.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/FYP/vami0001/.conda/envs/RJupyter/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading torch-2.9.0-cp314-cp314-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:01:41\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m  \u001b[33m0:01:07\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m  \u001b[33m0:01:19\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:21\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:29\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:35\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0ms\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.0-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m  \u001b[33m0:00:18\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mta \u001b[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading scikit_learn-1.7.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (248 kB)\n",
      "Downloading yarl-1.22.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (372 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp314-cp314-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.4-cp314-cp314-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (201 kB)\n",
      "Downloading pyarrow-22.0.0-cp314-cp314-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading scipy-1.16.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.3.3-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0ms\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (194 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, xxhash, triton, tqdm, threadpoolctl, sympy, safetensors, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, joblib, hf-xet, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, scipy, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, aiosignal, tokenizers, scikit-learn, nvidia-cusolver-cu12, aiohttp, transformers, torch, datasets, accelerate, peft\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49/49\u001b[0m [peft]8/49\u001b[0m [peft]7/49\u001b[0m [accelerate]sets]ers]]u12]12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.3.0 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 hf-xet-1.2.0 huggingface-hub-0.36.0 joblib-1.5.2 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.16 networkx-3.5 numpy-2.3.4 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 pandas-2.3.3 peft-0.17.1 propcache-0.4.1 pyarrow-22.0.0 regex-2025.10.23 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.3 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.22.1 torch-2.9.0 tqdm-4.67.1 transformers-4.57.1 triton-3.5.0 xxhash-3.6.0 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers peft accelerate datasets tqdm scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03743b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install peft --upgrade\n",
    "import peft\n",
    "#print(\"PEFT version:\", peft.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9017233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "#print(f\"Current working directory: {notebook_dir}\")\n",
    "DATA_DIR = os.path.join(notebook_dir, \"data\")\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sample = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "TEST_CSV  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "\n",
    "BASE_MODEL_NAME = \"roberta-base\"   # can downgrade to \"roberta-base\" if VRAM explodes\n",
    "MAX_LEN = 512                       # keep this reasonable for your GPU\n",
    "BATCH_SIZE = 2                      # we use grad accumulation instead of big batch\n",
    "GRAD_ACCUM_STEPS = 8                # effective batch_size ~= 16\n",
    "LR = 2e-5\n",
    "EPOCHS = 2\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "OUT_DIR = \"reward_teacher_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c81176b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  len_a  len_b\n",
      "0      0   4538   1206\n",
      "1      1   3114   3649\n",
      "2      2    921   1835\n",
      "3      0   3182   1562\n",
      "4      1   1300    772\n",
      "class counts:\n",
      " label\n",
      "0    20064\n",
      "1    19652\n",
      "2    17761\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# sanity: make one label column 0/1/2\n",
    "def row_label(r):\n",
    "    if r[\"winner_model_a\"] == 1:\n",
    "        return 0  # A preferred\n",
    "    elif r[\"winner_model_b\"] == 1:\n",
    "        return 1  # B preferred\n",
    "    else:\n",
    "        return 2  # tie\n",
    "\n",
    "df[\"label\"] = df.apply(row_label, axis=1)\n",
    "\n",
    "# We also keep lengths if you want analysis later\n",
    "df[\"len_a\"] = df[\"response_a\"].astype(str).str.len()\n",
    "df[\"len_b\"] = df[\"response_b\"].astype(str).str.len()\n",
    "\n",
    "# groups for CV: group by prompt (hash to avoid giant strings)\n",
    "df[\"prompt_group\"] = df[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9))\n",
    "print(df[[\"label\",\"len_a\",\"len_b\"]].head())\n",
    "print(\"class counts:\\n\", df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34a15403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwisePrefDataset(Dataset):\n",
    "    def __init__(self, df_fold, tokenizer, max_len=512):\n",
    "        self.df = df_fold.reset_index(drop=True)\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def encode_pair(self, prompt, resp):\n",
    "        # We'll just concat prompt + </s> + resp style for RoBERTa:\n",
    "        text = f\"Prompt: {prompt}\\nAnswer:\\n{resp}\"\n",
    "        return self.tok(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        enc_a = self.encode_pair(row[\"prompt\"], row[\"response_a\"])\n",
    "        enc_b = self.encode_pair(row[\"prompt\"], row[\"response_b\"])\n",
    "        label = row[\"label\"]\n",
    "        return {\n",
    "            \"input_ids_a\": enc_a[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_a\": enc_a[\"attention_mask\"].squeeze(0),\n",
    "            \"input_ids_b\": enc_b[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask_b\": enc_b[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "            \"row_id\": torch.tensor(row[\"id\"], dtype=torch.long),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef49d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model ready on cpu\n"
     ]
    }
   ],
   "source": [
    "#draft one dont need to run now\n",
    "'''from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "class RewardScorer(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(base_model_name)\n",
    "        self.config = self.backbone.config   # <--- this fixes the AttributeError\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.score_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_state = out.last_hidden_state[:, 0, :]   # CLS embedding\n",
    "        score = self.score_head(cls_state)           # [batch, 1]\n",
    "        return score.squeeze(-1)                     # [batch]\n",
    "\n",
    "def build_lora_reward_model(base_model_name):\n",
    "    base = RewardScorer(base_model_name)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\n",
    "            \"query\", \"key\", \"value\", \"dense\", \"out_proj\"\n",
    "        ],  # common layers in RoBERTa’s self-attn + FFN\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, peft_config)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "BASE_MODEL_NAME = \"roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "print(\" Model ready on\", DEVICE)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35404b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,678,784 || all params: 127,325,185 || trainable%: 2.1039\n",
      "✅ Model ready on cuda\n",
      "Scores: tensor([0.1878, 0.1746], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ✅ FINAL REWARD SCORER + LoRA BUILDER (100% stable)\n",
    "# ===============================================================\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# RewardScorer — scalar scoring head for reward modeling\n",
    "# ---------------------------------------------------------------\n",
    "class RewardScorer(nn.Module):\n",
    "    def __init__(self, base_model_name):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(base_model_name)\n",
    "        self.config = self.backbone.config\n",
    "        hidden_size = self.config.hidden_size\n",
    "        self.score_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # ✅ Fix: remove dummy token_type_ids buffer from RoBERTa\n",
    "        if hasattr(self.backbone.embeddings, \"token_type_ids\"):\n",
    "            del self.backbone.embeddings.token_type_ids\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        # ✅ Clean up unexpected kwargs from PEFT\n",
    "        kwargs.pop(\"labels\", None)\n",
    "        kwargs.pop(\"output_attentions\", None)\n",
    "        kwargs.pop(\"output_hidden_states\", None)\n",
    "        kwargs.pop(\"return_dict\", None)\n",
    "\n",
    "        # ✅ token_type_ids=None ensures no buffer expansion bug\n",
    "        out = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=None,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # CLS token embedding (RoBERTa uses <s> as CLS)\n",
    "        cls_state = out.last_hidden_state[:, 0, :]\n",
    "        score = self.score_head(cls_state)\n",
    "        return score.squeeze(-1)  # shape: [batch]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Build model + apply LoRA adapters\n",
    "# ---------------------------------------------------------------\n",
    "def build_lora_reward_model(base_model_name):\n",
    "    base = RewardScorer(base_model_name)\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query\", \"key\", \"value\", \"dense\", \"out_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"FEATURE_EXTRACTION\",  # prevents PEFT from adding 'labels'\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, peft_config)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Example usage / sanity check\n",
    "# ---------------------------------------------------------------\n",
    "BASE_MODEL_NAME = \"roberta-base\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "if hasattr(model, \"print_trainable_parameters\"):\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "print(f\"✅ Model ready on {DEVICE}\")\n",
    "\n",
    "# Quick smoke test\n",
    "inputs = tokenizer(\n",
    "    [\"hello world\", \"great job!\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scores = model(**inputs)\n",
    "    print(\"Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee8e7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pref_loss(scores_a, scores_b, labels, margin=0.5, tie_band=0.2):\n",
    "    # scores_a, scores_b: [batch]\n",
    "    # labels: [batch] in {0,1,2}\n",
    "    loss_list = []\n",
    "\n",
    "    # A wins -> sA >= sB + margin\n",
    "    mask_a = (labels == 0)\n",
    "    if mask_a.any():\n",
    "        diff = scores_b[mask_a] + margin - scores_a[mask_a]\n",
    "        # want diff <= 0\n",
    "        loss_a = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_a)\n",
    "\n",
    "    # B wins -> sB >= sA + margin\n",
    "    mask_b = (labels == 1)\n",
    "    if mask_b.any():\n",
    "        diff = scores_a[mask_b] + margin - scores_b[mask_b]\n",
    "        loss_b = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_b)\n",
    "\n",
    "    # Tie -> |sA - sB| <= tie_band\n",
    "    mask_t = (labels == 2)\n",
    "    if mask_t.any():\n",
    "        diff = torch.abs(scores_a[mask_t] - scores_b[mask_t]) - tie_band\n",
    "        loss_t = torch.clamp(diff, min=0).mean()\n",
    "        loss_list.append(loss_t)\n",
    "\n",
    "    if len(loss_list) == 0:\n",
    "        return torch.tensor(0.0, device=scores_a.device, requires_grad=True)\n",
    "    return torch.stack(loss_list).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31c9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1: 100%|██████████| 22991/22991 [53:41<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss (divided): 0.0377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval fold 1: 100%|██████████| 5748/5748 [05:17<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold 2 Epoch 0: 100%|██████████| 22991/22991 [53:35<00:00,  7.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 avg loss (divided): 0.0395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1:  95%|█████████▍| 21820/22991 [50:51<02:42,  7.22it/s] "
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "folds = []\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(gkf.split(df, groups=df[\"prompt_group\"])):\n",
    "    df.loc[va_idx, \"fold\"] = fold_id\n",
    "df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "\n",
    "def train_one_fold(fold_id):\n",
    "    print(f\"\\n===== FOLD {fold_id} =====\")\n",
    "    df_tr = df[df[\"fold\"] != fold_id].reset_index(drop=True)\n",
    "    df_va = df[df[\"fold\"] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    train_ds = PairwisePrefDataset(df_tr, tokenizer, max_len=MAX_LEN)\n",
    "    va_ds    = PairwisePrefDataset(df_va, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_loader    = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # simple linear warmup + cosine cooldown\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS) * EPOCHS\n",
    "    warmup_steps = min(100, total_steps // 10)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(1, warmup_steps)\n",
    "        # cosine decay after warmup\n",
    "        progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold_id} Epoch {epoch}\")):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a)\n",
    "            scores_b = model(input_ids_b, attn_b)\n",
    "\n",
    "            loss = pref_loss(scores_a, scores_b, labels)\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        print(f\"Epoch {epoch} avg loss (divided): {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # after training, eval on val fold to get a feel\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_ids    = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(va_loader, desc=f\"Eval fold {fold_id}\"):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "            row_ids     = batch[\"row_id\"].cpu().numpy()\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a).cpu().numpy()\n",
    "            scores_b = model(input_ids_b, attn_b).cpu().numpy()\n",
    "\n",
    "            all_scores.append(np.stack([scores_a, scores_b], axis=1))  # shape [batch,2]\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_ids.append(row_ids)\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0)   # [N_val,2]\n",
    "    all_labels = np.concatenate(all_labels, axis=0)   # [N_val]\n",
    "    all_ids    = np.concatenate(all_ids, axis=0)      # [N_val]\n",
    "\n",
    "    # save checkpoint\n",
    "    fold_dir = os.path.join(OUT_DIR, f\"fold_{fold_id}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(fold_dir, \"reward_teacher_roberta.pt\"))\n",
    "\n",
    "    # we also save the val fold scores for later calibration/logit building\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"score_a\": all_scores[:,0],\n",
    "        \"score_b\": all_scores[:,1],\n",
    "        \"label\": all_labels,\n",
    "        \"fold\": fold_id\n",
    "    })\n",
    "    oof_df.to_csv(os.path.join(fold_dir, \"oof_scores.csv\"), index=False)\n",
    "\n",
    "    return oof_df\n",
    "\n",
    "all_oof = []\n",
    "for f in range(N_FOLDS):\n",
    "    oof_df = train_one_fold(f)\n",
    "    all_oof.append(oof_df)\n",
    "\n",
    "all_oof = pd.concat(all_oof, ignore_index=True)\n",
    "all_oof.to_csv(os.path.join(OUT_DIR, \"all_oof_scores.csv\"), index=False)\n",
    "print(\"Saved all OOF scores to\", os.path.join(OUT_DIR, \"all_oof_scores.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de32d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fold 0 already completed — skipping.\n",
      " Fold 1 already completed — skipping.\n",
      "\n",
      "===== FOLD 2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold 2 Epoch 0: 100%|██████████| 22991/22991 [1:10:47<00:00,  5.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 avg loss (divided): 0.0397\n",
      " Saved checkpoint: reward_teacher_outputs/fold_2/epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2 Epoch 1: 100%|██████████| 22991/22991 [1:10:54<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss (divided): 0.0381\n",
      " Saved checkpoint: reward_teacher_outputs/fold_2/epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval fold 2: 100%|██████████| 5748/5748 [08:01<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 3 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold 3 Epoch 0: 100%|██████████| 22991/22991 [1:09:47<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 avg loss (divided): 0.0394\n",
      " Saved checkpoint: reward_teacher_outputs/fold_3/epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3 Epoch 1: 100%|██████████| 22991/22991 [1:03:44<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss (divided): 0.0375\n",
      " Saved checkpoint: reward_teacher_outputs/fold_3/epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval fold 3: 100%|██████████| 5748/5748 [06:49<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FOLD 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fold 4 Epoch 0:  39%|███▊      | 8875/22991 [24:35<38:46,  6.07it/s]  "
     ]
    }
   ],
   "source": [
    "# just repeated section of the code to continue with training \n",
    "# this has checkpoints logged so keep this code \n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "folds = []\n",
    "for fold_id, (tr_idx, va_idx) in enumerate(gkf.split(df, groups=df[\"prompt_group\"])):\n",
    "    df.loc[va_idx, \"fold\"] = fold_id\n",
    "df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "# ==========================================================\n",
    "# TRAINING LOOP — now resume-safe (per-epoch save + skip done folds)\n",
    "# ==========================================================\n",
    "\n",
    "def train_one_fold(fold_id):\n",
    "    print(f\"\\n===== FOLD {fold_id} =====\")\n",
    "    df_tr = df[df[\"fold\"] != fold_id].reset_index(drop=True)\n",
    "    df_va = df[df[\"fold\"] == fold_id].reset_index(drop=True)\n",
    "\n",
    "    train_ds = PairwisePrefDataset(df_tr, tokenizer, max_len=MAX_LEN)\n",
    "    va_ds    = PairwisePrefDataset(df_va, tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_loader    = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # scheduler: linear warmup + cosine decay\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS) * EPOCHS\n",
    "    warmup_steps = min(100, total_steps // 10)\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / max(1, warmup_steps)\n",
    "        progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
    "        return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold_id} Epoch {epoch}\")):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a)\n",
    "            scores_b = model(input_ids_b, attn_b)\n",
    "\n",
    "            loss = pref_loss(scores_a, scores_b, labels)\n",
    "            loss = loss / GRAD_ACCUM_STEPS\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch} avg loss (divided): {avg_loss:.4f}\")\n",
    "\n",
    "        #  Save checkpoint after each epoch\n",
    "        fold_dir = os.path.join(OUT_DIR, f\"fold_{fold_id}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        ckpt_path = os.path.join(fold_dir, f\"epoch_{epoch}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\" Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # ============================\n",
    "    # Evaluation on validation set\n",
    "    # ============================\n",
    "    model.eval()\n",
    "    all_scores, all_labels, all_ids = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(va_loader, desc=f\"Eval fold {fold_id}\"):\n",
    "            input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "            attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "            input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "            attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "            labels      = batch[\"label\"].to(DEVICE)\n",
    "            row_ids     = batch[\"row_id\"].cpu().numpy()\n",
    "\n",
    "            scores_a = model(input_ids_a, attn_a).cpu().numpy()\n",
    "            scores_b = model(input_ids_b, attn_b).cpu().numpy()\n",
    "\n",
    "            all_scores.append(np.stack([scores_a, scores_b], axis=1))\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_ids.append(row_ids)\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_ids    = np.concatenate(all_ids, axis=0)\n",
    "\n",
    "    # save final fold model + validation results\n",
    "    torch.save(model.state_dict(), os.path.join(fold_dir, \"reward_teacher_roberta.pt\"))\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": all_ids,\n",
    "        \"score_a\": all_scores[:, 0],\n",
    "        \"score_b\": all_scores[:, 1],\n",
    "        \"label\": all_labels,\n",
    "        \"fold\": fold_id\n",
    "    })\n",
    "    oof_df.to_csv(os.path.join(fold_dir, \"oof_scores.csv\"), index=False)\n",
    "    return oof_df\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# MAIN LOOP — skip completed folds automatically\n",
    "# ==========================================================\n",
    "all_oof = []\n",
    "for f in range(N_FOLDS):\n",
    "    fold_dir = os.path.join(OUT_DIR, f\"fold_{f}\")\n",
    "    final_ckpt = os.path.join(fold_dir, \"reward_teacher_roberta.pt\")\n",
    "    if os.path.exists(final_ckpt):\n",
    "        print(f\" Fold {f} already completed — skipping.\")\n",
    "        continue\n",
    "\n",
    "    oof_df = train_one_fold(f)\n",
    "    all_oof.append(oof_df)\n",
    "\n",
    "if all_oof:\n",
    "    all_oof = pd.concat(all_oof, ignore_index=True)\n",
    "    all_oof.to_csv(os.path.join(OUT_DIR, \"all_oof_scores.csv\"), index=False)\n",
    "    print(\" Saved all OOF scores to\", os.path.join(OUT_DIR, \"all_oof_scores.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbde4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# map label int -> onehot [A,B,Tie]\n",
    "def label_to_onehot(y):\n",
    "    out = np.zeros((len(y),3), dtype=np.float32)\n",
    "    for i,lab in enumerate(y):\n",
    "        out[i, lab] = 1.0\n",
    "    return out\n",
    "\n",
    "oof = pd.read_csv(os.path.join(OUT_DIR, \"all_oof_scores.csv\"))\n",
    "\n",
    "y_true = label_to_onehot(oof[\"label\"].values)\n",
    "sa = oof[\"score_a\"].values\n",
    "sb = oof[\"score_b\"].values\n",
    "\n",
    "def probs_from_scores(sa, sb, temp, tie_alpha):\n",
    "    # Bradley-Terry-ish:\n",
    "    # pA_raw = exp(sa/temp)\n",
    "    # pB_raw = exp(sb/temp)\n",
    "    # tie_raw = exp(-abs(sa-sb)*tie_alpha)\n",
    "    pA_raw = np.exp(sa / temp)\n",
    "    pB_raw = np.exp(sb / temp)\n",
    "    tie_raw = np.exp(-np.abs(sa - sb) * tie_alpha)\n",
    "\n",
    "    denom = pA_raw + pB_raw + tie_raw + 1e-9\n",
    "    pA = pA_raw / denom\n",
    "    pB = pB_raw / denom\n",
    "    pT = tie_raw / denom\n",
    "    return np.stack([pA,pB,pT], axis=1)\n",
    "\n",
    "def logloss(params):\n",
    "    temp = np.exp(params[0])          # >0\n",
    "    tie_alpha = np.exp(params[1])     # >0\n",
    "    pred = probs_from_scores(sa, sb, temp, tie_alpha)\n",
    "    # clip for stability\n",
    "    pred = np.clip(pred, 1e-7, 1-1e-7)\n",
    "    return -np.mean((y_true * np.log(pred)).sum(axis=1))\n",
    "\n",
    "res = opt.minimize(logloss, x0=[0.0, 0.0], method=\"Nelder-Mead\")\n",
    "print(\"opt result:\", res)\n",
    "\n",
    "best_temp = math.exp(res.x[0])\n",
    "best_tie_alpha = math.exp(res.x[1])\n",
    "print(\"best_temp =\", best_temp, \" best_tie_alpha =\", best_tie_alpha)\n",
    "\n",
    "# save calibration params\n",
    "calib_path = os.path.join(OUT_DIR, \"calibration_params.json\")\n",
    "import json\n",
    "with open(calib_path, \"w\") as f:\n",
    "    json.dump({\"temp\": best_temp, \"tie_alpha\": best_tie_alpha}, f)\n",
    "print(\"Saved\", calib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"calibration_params.json\")) as f:\n",
    "    calib = json.load(f)\n",
    "TEMP = calib[\"temp\"]\n",
    "TIE_ALPHA = calib[\"tie_alpha\"]\n",
    "\n",
    "def get_model_for_fold(fold_id):\n",
    "    m = build_lora_reward_model(BASE_MODEL_NAME).to(DEVICE)\n",
    "    state_path = os.path.join(OUT_DIR, f\"fold_{fold_id}\", \"reward_teacher_roberta.pt\")\n",
    "    m.load_state_dict(torch.load(state_path, map_location=DEVICE))\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "def score_pairs_df(df_any, max_len=MAX_LEN):\n",
    "    ds = PairwisePrefDataset(df_any, tokenizer, max_len=max_len)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    fold_scores_a = []\n",
    "    fold_scores_b = []\n",
    "    with torch.no_grad():\n",
    "        for fold_id in range(N_FOLDS):\n",
    "            model_f = get_model_for_fold(fold_id)\n",
    "            sa_list, sb_list = [], []\n",
    "            for batch in tqdm(loader, desc=f\"Infer fold {fold_id}\"):\n",
    "                input_ids_a = batch[\"input_ids_a\"].to(DEVICE)\n",
    "                attn_a      = batch[\"attention_mask_a\"].to(DEVICE)\n",
    "                input_ids_b = batch[\"input_ids_b\"].to(DEVICE)\n",
    "                attn_b      = batch[\"attention_mask_b\"].to(DEVICE)\n",
    "\n",
    "                scores_a = model_f(input_ids_a, attn_a).cpu().numpy()\n",
    "                scores_b = model_f(input_ids_b, attn_b).cpu().numpy()\n",
    "                sa_list.append(scores_a)\n",
    "                sb_list.append(scores_b)\n",
    "\n",
    "            sa_all = np.concatenate(sa_list)\n",
    "            sb_all = np.concatenate(sb_list)\n",
    "            fold_scores_a.append(sa_all)\n",
    "            fold_scores_b.append(sb_all)\n",
    "\n",
    "    # average across folds\n",
    "    sa_mean = np.mean(np.stack(fold_scores_a, axis=0), axis=0)\n",
    "    sb_mean = np.mean(np.stack(fold_scores_b, axis=0), axis=0)\n",
    "    return sa_mean, sb_mean\n",
    "\n",
    "# score train\n",
    "sa_train, sb_train = score_pairs_df(df)\n",
    "# score test\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "df_test[\"label\"] = -1  # dummy\n",
    "df_test[\"prompt_group\"] = df_test[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9))\n",
    "sa_test, sb_test = score_pairs_df(df_test)\n",
    "\n",
    "def probs_from_scores_numpy(sa, sb, temp, tie_alpha):\n",
    "    pA_raw = np.exp(sa / temp)\n",
    "    pB_raw = np.exp(sb / temp)\n",
    "    tie_raw = np.exp(-np.abs(sa - sb) * tie_alpha)\n",
    "    denom = pA_raw + pB_raw + tie_raw + 1e-9\n",
    "    pA = pA_raw / denom\n",
    "    pB = pB_raw / denom\n",
    "    pT = tie_raw / denom\n",
    "    return np.stack([pA,pB,pT], axis=1)\n",
    "\n",
    "probs_train = probs_from_scores_numpy(sa_train, sb_train, TEMP, TIE_ALPHA)\n",
    "probs_test  = probs_from_scores_numpy(sa_test,  sb_test,  TEMP, TIE_ALPHA)\n",
    "\n",
    "teacher_train_out = pd.DataFrame({\n",
    "    \"id\": df[\"id\"].values,\n",
    "    \"pA\": probs_train[:,0],\n",
    "    \"pB\": probs_train[:,1],\n",
    "    \"pTie\": probs_train[:,2],\n",
    "    \"label\": df[\"label\"].values\n",
    "})\n",
    "teacher_test_out = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"].values,\n",
    "    \"pA\": probs_test[:,0],\n",
    "    \"pB\": probs_test[:,1],\n",
    "    \"pTie\": probs_test[:,2],\n",
    "})\n",
    "\n",
    "teacher_train_out.to_csv(os.path.join(OUT_DIR, \"teacher_logits_train.csv\"), index=False)\n",
    "teacher_test_out.to_csv(os.path.join(OUT_DIR, \"teacher_logits_test.csv\"), index=False)\n",
    "\n",
    "print(\"Saved distillation targets:\")\n",
    "print(\" -\", os.path.join(OUT_DIR, \"teacher_logits_train.csv\"))\n",
    "print(\" -\", os.path.join(OUT_DIR, \"teacher_logits_test.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ece3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
