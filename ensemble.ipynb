{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e34b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What this does:\\n- Loads train.csv (labels + prompt for GroupKFold) and test.csv\\n- Loads per-model OOF train probs and test probs (any subset present)\\n  Expected columns in each predictions file: id, pA, pB, pTie  (probabilities)\\n- Rigorously validates inputs (alignment, ranges, sums, NaNs, class order)\\n- Builds a meta-dataset (concatenate per-model probs + engineered margins/entropy/lengths)\\n- Runs GroupKFold(by prompt) CV with two strategies:\\n    1) Constrained non-negative, sum-to-1 **logit-space** weighted average\\n    2) Multinomial Logistic Regression (stacking) on features\\n  Chooses the strategy with lower mean CV log-loss\\n- Refit on full train, predict test, and write:\\n    runs/<tag>/submission.csv  (id,winner_model_a,winner_model_b,winner_tie)\\n    runs/<tag>/oof_combined.csv\\n    runs/<tag>/cv_report.json\\n    runs/<tag>/blend_config.json\\n- Includes symmetry unit-tests (swap A/B) and tie calibration checks\\n- Gracefully degrades if some model files are missing'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"What this does:\n",
    "- Loads train.csv (labels + prompt for GroupKFold) and test.csv\n",
    "- Loads per-model OOF train probs and test probs (any subset present)\n",
    "  Expected columns in each predictions file: id, pA, pB, pTie  (probabilities)\n",
    "- Rigorously validates inputs (alignment, ranges, sums, NaNs, class order)\n",
    "- Builds a meta-dataset (concatenate per-model probs + engineered margins/entropy/lengths)\n",
    "- Runs GroupKFold(by prompt) CV with two strategies:\n",
    "    1) Constrained non-negative, sum-to-1 **logit-space** weighted average\n",
    "    2) Multinomial Logistic Regression (stacking) on features\n",
    "  Chooses the strategy with lower mean CV log-loss\n",
    "- Refit on full train, predict test, and write:\n",
    "    runs/<tag>/submission.csv  (id,winner_model_a,winner_model_b,winner_tie)\n",
    "    runs/<tag>/oof_combined.csv\n",
    "    runs/<tag>/cv_report.json\n",
    "    runs/<tag>/blend_config.json\n",
    "- Includes symmetry unit-tests (swap A/B) and tie calibration checks\n",
    "- Gracefully degrades if some model files are missing\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696ca69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import copy\n",
    "import argparse\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab7fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.special import logit, expit\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a4d707",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b773ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"train_csv\": \"./data/train.csv\",\n",
    "    \"test_csv\": \"./data/test.csv\",\n",
    "\n",
    "    # === All models to blend ===\n",
    "    \"model_specs\": [\n",
    "        {\n",
    "            \"name\": \"teacher2_reward\",   # Teacher 2: Reward model\n",
    "            \"train_path\": \"./teacher_logits_train_improved.csv\",\n",
    "            \"test_path\":  \"./teacher_logits_test_improved.csv\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"deberta_head_only\", # Teacher 1: Cross-encoder / DeBERTa baseline\n",
    "            \"train_path\": \"./deberta_oof_train.csv\",\n",
    "            \"test_path\":  \"./submission_deberta_small_calibrated.csv\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"student_model\",     # ðŸ§  Student: Distilled DeBERTa-v3-base\n",
    "            \"train_path\": \"./student_outputs/student_oof_probs.csv\",\n",
    "            \"test_path\":  \"./student_outputs/student_test_probs.csv\",\n",
    "        },\n",
    "    ],\n",
    "\n",
    "    # === NEW output folder for the final ensemble ===\n",
    "    \"runs_dir\": \"./runs_final_ensemble\",   # <--- ðŸ‘ˆ This creates a separate folder\n",
    "\n",
    "    \"n_folds\": 5,\n",
    "    \"random_state\": 42,\n",
    "\n",
    "    # === blending & feature flags ===\n",
    "    \"use_length_features\": False,\n",
    "    \"use_weighted_logit_blend\": True,\n",
    "    \"use_stacking_lr\": True,\n",
    "    \"stack_use_engineered_features\": True,\n",
    "\n",
    "    # === numeric constants ===\n",
    "    \"epsilon\": 1e-6,\n",
    "\n",
    "    # === optimizer ===\n",
    "    \"weight_init\": \"equal\",\n",
    "    \"weight_opt_max_iter\": 800,\n",
    "    \"weight_opt_tolerance\": 1e-8,\n",
    "\n",
    "    # === stacking config ===\n",
    "    \"stack_C\": 2.0,\n",
    "    \"stack_max_iter\": 500,\n",
    "    \"stack_solver\": \"lbfgs\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecda30f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# LOGGING\n",
    "# ---------------------------------\n",
    "def setup_logging():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"[%(asctime)s] %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5879bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7737d12b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# UTILS\n",
    "# ---------------------------------\n",
    "def softmax(logits: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    x = logits - logits.max(axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / (e.sum(axis=axis, keepdims=True) + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23032195",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def probs_to_logits(p: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multi-class safe logit transform: returns class-wise log-probs (unnormalized).\n",
    "    For blending we prefer to work in (natural) logit space per class via log(p).\n",
    "    Here we use log(p) and renormalize later with softmax after weighting.\n",
    "\n",
    "    p: [..., 3] (A,B,Tie) probs\n",
    "    \"\"\"\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969a2edc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def logits_to_probs(logits: np.ndarray) -> np.ndarray:\n",
    "    return softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a045cdd3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_prob_frame(df: pd.DataFrame, name: str):\n",
    "    required = [\"id\", \"pA\", \"pB\", \"pTie\"]\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"{name}: missing column '{c}'\")\n",
    "    vals = df[[\"pA\", \"pB\", \"pTie\"]].values\n",
    "    if np.isnan(vals).any() or np.isinf(vals).any():\n",
    "        raise ValueError(f\"{name}: found NaN/Inf probabilities.\")\n",
    "    row_sums = vals.sum(axis=1)\n",
    "    bad = np.where((row_sums < 0.999 - 1e-5) | (row_sums > 1.001 + 1e-5))[0]\n",
    "    if len(bad) > 0:\n",
    "        logging.warning(f\"{name}: {len(bad)} rows do not sum to ~1. Will renormalize softly.\")\n",
    "        # gentle renorm\n",
    "        vals = np.clip(vals, 0, 1)\n",
    "        vals = vals / (vals.sum(axis=1, keepdims=True) + 1e-12)\n",
    "        df[[\"pA\", \"pB\", \"pTie\"]] = vals\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19dac0df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def safe_inner_merge(a: pd.DataFrame, b: pd.DataFrame, on: str, name_a: str, name_b: str) -> pd.DataFrame:\n",
    "    before = len(a)\n",
    "    out = a.merge(b, on=on, how=\"inner\", suffixes=(\"\", f\"__{name_b}\"))\n",
    "    after = len(out)\n",
    "    if after < before:\n",
    "        logging.warning(f\"Merge({name_a} x {name_b}) dropped {before-after} rows (inner join on '{on}').\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69342130",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def label_from_winners(r: pd.Series) -> int:\n",
    "    # 0 = A, 1 = B, 2 = Tie\n",
    "    if r[\"winner_model_a\"] == 1:\n",
    "        return 0\n",
    "    if r[\"winner_model_b\"] == 1:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34733a2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def entropy_rows(p: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    return -(p * np.log(p)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ee33d3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def make_prompt_groups(df: pd.DataFrame) -> np.ndarray:\n",
    "    # Stable hash for GroupKFold by prompt\n",
    "    return df[\"prompt\"].astype(str).apply(lambda x: hash(x) % (10**9)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf445a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def ab_swap(df_probs: pd.DataFrame) -> pd.DataFrame:\n",
    "    swapped = df_probs.copy()\n",
    "    swapped = swapped.rename(columns={\"pA\": \"pB_temp\", \"pB\": \"pA_temp\"})\n",
    "    swapped[\"pA\"] = swapped[\"pA_temp\"]\n",
    "    swapped[\"pB\"] = swapped[\"pB_temp\"]\n",
    "    swapped.drop(columns=[\"pA_temp\", \"pB_temp\"], inplace=True)\n",
    "    # Tie unchanged\n",
    "    return swapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f88fc4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf2924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# LOADING DATA\n",
    "# ---------------------------------\n",
    "def load_official(train_csv: str, test_csv: str, use_len_feats: bool) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train = pd.read_csv(train_csv)\n",
    "    test  = pd.read_csv(test_csv)\n",
    "    # labels 0/1/2\n",
    "    train[\"label\"] = train.apply(label_from_winners, axis=1)\n",
    "\n",
    "    # lengths (optional)\n",
    "    if use_len_feats:\n",
    "        for df in (train, test):\n",
    "            for c in (\"response_a\", \"response_b\", \"prompt\"):\n",
    "                df[f\"{c}_len\"] = df[c].astype(str).str.len()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d31cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelPreds:\n",
    "    name: str\n",
    "    train: Optional[pd.DataFrame]\n",
    "    test:  Optional[pd.DataFrame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "671dfe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def try_load_model_preds(spec: Dict) -> ModelPreds:\n",
    "    name = spec[\"name\"]\n",
    "    tr_path = spec.get(\"train_path\", None)\n",
    "    te_path = spec.get(\"test_path\", None)\n",
    "\n",
    "    tr_df = None\n",
    "    te_df = None\n",
    "    if tr_path and os.path.exists(tr_path):\n",
    "        tr_df = pd.read_csv(tr_path)\n",
    "        tr_df = check_prob_frame(tr_df, f\"{name}::train\")\n",
    "    else:\n",
    "        logging.warning(f\"[{name}] OOF train file not found -> skipping train for this model.\")\n",
    "\n",
    "    if te_path and os.path.exists(te_path):\n",
    "        te_df = pd.read_csv(te_path)\n",
    "        te_df = check_prob_frame(te_df, f\"{name}::test\")\n",
    "    else:\n",
    "        logging.warning(f\"[{name}] test file not found -> skipping test for this model.\")\n",
    "\n",
    "    return ModelPreds(name=name, train=tr_df, test=te_df)'''\n",
    "\n",
    "def try_load_model_preds(spec: Dict) -> ModelPreds:\n",
    "    name = spec[\"name\"]\n",
    "    tr_path = spec.get(\"train_path\", None)\n",
    "    te_path = spec.get(\"test_path\", None)\n",
    "\n",
    "    tr_df = None\n",
    "    te_df = None\n",
    "\n",
    "    if tr_path and os.path.exists(tr_path):\n",
    "        tr_df = pd.read_csv(tr_path)\n",
    "    else:\n",
    "        logging.warning(f\"[{name}] OOF train file not found -> skipping train for this model.\")\n",
    "\n",
    "    if te_path and os.path.exists(te_path):\n",
    "        te_df = pd.read_csv(te_path)\n",
    "    else:\n",
    "        logging.warning(f\"[{name}] test file not found -> skipping test for this model.\")\n",
    "\n",
    "    #Standardize column names so DeBERTa submissions work\n",
    "    rename_map = {\n",
    "        \"winner_model_a\": \"pA\",\n",
    "        \"winner_model_b\": \"pB\",\n",
    "        \"winner_tie\": \"pTie\"\n",
    "    }\n",
    "    if tr_df is not None:\n",
    "        tr_df.rename(columns=rename_map, inplace=True)\n",
    "    if te_df is not None:\n",
    "        te_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "    # Run safety checks if applicable\n",
    "    if tr_df is not None:\n",
    "        tr_df = check_prob_frame(tr_df, f\"{name}::train\")\n",
    "    if te_df is not None:\n",
    "        te_df = check_prob_frame(te_df, f\"{name}::test\")\n",
    "\n",
    "    return ModelPreds(name=name, train=tr_df, test=te_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eaa95357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# FEATURE ENGINEERING\n",
    "# ---------------------------------\n",
    "def build_feature_matrices(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    preds_list: List[ModelPreds],\n",
    "    use_engineered: bool = True,\n",
    "    use_len_feats: bool = True,\n",
    "    eps: float = 1e-9,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      X_train, X_test, feat_names\n",
    "    Each contains concatenated per-model probability channels + engineered features.\n",
    "    \"\"\"\n",
    "    base_cols_train = [\"id\", \"label\", \"prompt\"]\n",
    "    if not set(base_cols_train).issubset(train_df.columns):\n",
    "        raise ValueError(\"train_df must contain id, label, prompt\")\n",
    "\n",
    "    # Start with the ID and target; weâ€™ll add features to side DataFrames and join by id\n",
    "    Xtr = train_df[[\"id\"]].copy()\n",
    "    Xte = test_df[[\"id\"]].copy()\n",
    "\n",
    "    feat_names: List[str] = []\n",
    "\n",
    "    # Concatenate all model probs with clear prefixes\n",
    "    for mp in preds_list:\n",
    "        if mp.train is None and mp.test is None:\n",
    "            continue\n",
    "        mname = mp.name\n",
    "        if mp.train is not None:\n",
    "            tmp = mp.train[[\"id\", \"pA\", \"pB\", \"pTie\"]].copy()\n",
    "            tmp = tmp.rename(columns={\"pA\": f\"{mname}_pA\", \"pB\": f\"{mname}_pB\", \"pTie\": f\"{mname}_pTie\"})\n",
    "            Xtr = safe_inner_merge(Xtr, tmp, on=\"id\", name_a=\"Xtr\", name_b=mname)\n",
    "        if mp.test is not None:\n",
    "            tmp = mp.test[[\"id\", \"pA\", \"pB\", \"pTie\"]].copy()\n",
    "            tmp = tmp.rename(columns={\"pA\": f\"{mname}_pA\", \"pB\": f\"{mname}_pB\", \"pTie\": f\"{mname}_pTie\"})\n",
    "            Xte = safe_inner_merge(Xte, tmp, on=\"id\", name_a=\"Xte\", name_b=mname)\n",
    "\n",
    "    # Track the per-model prob features\n",
    "    for mp in preds_list:\n",
    "        mname = mp.name\n",
    "        for ch in [\"pA\", \"pB\", \"pTie\"]:\n",
    "            col = f\"{mname}_{ch}\"\n",
    "            if col in Xtr.columns:\n",
    "                feat_names.append(col)\n",
    "\n",
    "    # Engineered features per model (margins, entropy, confidence)\n",
    "    if use_engineered:\n",
    "        for mp in preds_list:\n",
    "            mname = mp.name\n",
    "            pAcol, pBcol, pTcol = f\"{mname}_pA\", f\"{mname}_pB\", f\"{mname}_pTie\"\n",
    "            if all(col in Xtr.columns for col in [pAcol, pBcol, pTcol]):\n",
    "                for df, label in [(Xtr, \"tr\"), (Xte, \"te\")]:\n",
    "                    # margins\n",
    "                    df[f\"{mname}_margin_AB\"] = df[pAcol] - df[pBcol]\n",
    "                    df[f\"{mname}_margin_maxTie\"] = df[pTcol] - np.maximum(df[pAcol], df[pBcol])\n",
    "                    # max prob & entropy\n",
    "                    probs = df[[pAcol, pBcol, pTcol]].values\n",
    "                    df[f\"{mname}_maxprob\"] = probs.max(axis=1)\n",
    "                    df[f\"{mname}_entropy\"] = entropy_rows(probs, eps=eps)\n",
    "                feat_names += [f\"{mname}_margin_AB\", f\"{mname}_margin_maxTie\", f\"{mname}_maxprob\", f\"{mname}_entropy\"]\n",
    "\n",
    "    # Length features (from raw train/test)\n",
    "    if use_len_feats:\n",
    "        for col in [\"response_a_len\", \"response_b_len\", \"prompt_len\"]:\n",
    "            if col in train_df.columns and col in test_df.columns:\n",
    "                Xtr[col] = train_df[col].values\n",
    "                Xte[col] = test_df[col].values\n",
    "                feat_names.append(col)\n",
    "        # simple interactions\n",
    "        if all(c in Xtr.columns for c in [\"response_a_len\", \"response_b_len\"]):\n",
    "            Xtr[\"len_sum\"] = Xtr[\"response_a_len\"] + Xtr[\"response_b_len\"]\n",
    "            Xtr[\"len_diff\"] = np.abs(Xtr[\"response_a_len\"] - Xtr[\"response_b_len\"])\n",
    "            Xte[\"len_sum\"] = Xte[\"response_a_len\"] + Xte[\"response_b_len\"]\n",
    "            Xte[\"len_diff\"] = np.abs(Xte[\"response_a_len\"] - Xte[\"response_b_len\"])\n",
    "            feat_names += [\"len_sum\", \"len_diff\"]\n",
    "\n",
    "    # Fill any residual NaNs (e.g., a model missing in test)\n",
    "    Xtr = Xtr.fillna(0.0)\n",
    "    Xte = Xte.fillna(0.0)\n",
    "\n",
    "    # Final safety: ensure all feat_names exist in Xtr/Xte\n",
    "    feat_names = [f for f in feat_names if f in Xtr.columns and f in Xte.columns]\n",
    "\n",
    "    return Xtr, Xte, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee59af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# EVALUATION / CV\n",
    "# ---------------------------------\n",
    "def cv_splits(train_df: pd.DataFrame, n_folds: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    groups = make_prompt_groups(train_df)\n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    splits = []\n",
    "    for tr_idx, va_idx in gkf.split(train_df, groups=groups):\n",
    "        splits.append((tr_idx, va_idx))\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5000c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# BLEND STRATEGY 1: Constrained logit-space weighted average\n",
    "# ---------------------------------\n",
    "@dataclass\n",
    "class WeightedLogitBlend:\n",
    "    model_names: List[str]\n",
    "    weights: Optional[np.ndarray] = None   # shape [n_models, 3] or [n_models] if shared across classes\n",
    "    classwise: bool = False                # if True, learn separate weights per class\n",
    "\n",
    "    def _init_weights(self, n_models: int, classwise: bool, init: str) -> np.ndarray:\n",
    "        if classwise:\n",
    "            if init == \"equal\":\n",
    "                W = np.ones((n_models, 3), dtype=np.float64) / n_models\n",
    "            elif init == \"prioritized_first\":\n",
    "                W = np.zeros((n_models, 3), dtype=np.float64)\n",
    "                W[0, :] = 1.0\n",
    "            else:  # random\n",
    "                R = np.random.RandomState(42).rand(n_models, 3)\n",
    "                W = R / (R.sum(axis=0, keepdims=True) + 1e-12)\n",
    "        else:\n",
    "            if init == \"equal\":\n",
    "                w = np.ones((n_models,), dtype=np.float64) / n_models\n",
    "            elif init == \"prioritized_first\":\n",
    "                w = np.zeros((n_models,), dtype=np.float64)\n",
    "                w[0] = 1.0\n",
    "            else:\n",
    "                R = np.random.RandomState(42).rand(n_models)\n",
    "                w = R / (R.sum() + 1e-12)\n",
    "            W = w\n",
    "        return W\n",
    "\n",
    "    def _param_to_weights(self, theta: np.ndarray, n_models: int, classwise: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Map unconstrained theta -> nonnegative weights that sum to 1 (per class if classwise)\n",
    "        Use softmax transform.\n",
    "        \"\"\"\n",
    "        if classwise:\n",
    "            theta_2d = theta.reshape(n_models, 3)\n",
    "            W = np.exp(theta_2d)\n",
    "            W = W / (W.sum(axis=0, keepdims=True) + 1e-12)\n",
    "        else:\n",
    "            w = np.exp(theta)\n",
    "            W = w / (w.sum() + 1e-12)\n",
    "        return W\n",
    "\n",
    "    def fit_cv(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        feat_names: List[str],\n",
    "        y: np.ndarray,\n",
    "        splits: List[Tuple[np.ndarray, np.ndarray]],\n",
    "        model_names_in_feats: List[str],\n",
    "        max_iter: int = 800,\n",
    "        tol: float = 1e-8,\n",
    "        eps: float = 1e-9,\n",
    "        init: str = \"equal\",\n",
    "        classwise: bool = False,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        X: contains stacked per-model prob features with names like \"{model}_pA/B/Tie\"\n",
    "        model_names_in_feats: order of models to stack; must exist in feat_names\n",
    "        \"\"\"\n",
    "        # gather per-model probs into 3D array: [N, n_models, 3]\n",
    "        N = len(X)\n",
    "        ms = model_names_in_feats\n",
    "        n_models = len(ms)\n",
    "        if n_models == 0:\n",
    "            raise ValueError(\"WeightedLogitBlend: no models to blend.\")\n",
    "\n",
    "        P = np.zeros((N, n_models, 3), dtype=np.float64)\n",
    "        for i, m in enumerate(ms):\n",
    "            P[:, i, 0] = X[f\"{m}_pA\"].values\n",
    "            P[:, i, 1] = X[f\"{m}_pB\"].values\n",
    "            P[:, i, 2] = X[f\"{m}_pTie\"].values\n",
    "\n",
    "        # convert probs to logits (per class)\n",
    "        L = np.log(np.clip(P, eps, 1 - eps))  # shape [N, M, 3]\n",
    "\n",
    "        cv_losses = []\n",
    "        oof = np.zeros((N, 3), dtype=np.float64)\n",
    "        best_thetas = []\n",
    "\n",
    "        # initial theta for optimization\n",
    "        if classwise:\n",
    "            W0 = self._init_weights(n_models, True, init)  # [M,3]\n",
    "            theta0 = np.log(np.clip(W0, 1e-8, 1.0))  # inverse softmax-ish seed\n",
    "            theta0 = theta0.ravel()\n",
    "        else:\n",
    "            W0 = self._init_weights(n_models, False, init)  # [M]\n",
    "            theta0 = np.log(np.clip(W0, 1e-8, 1.0))  # seed\n",
    "\n",
    "        def fold_objective(theta, L_tr, y_tr):\n",
    "            W = self._param_to_weights(theta, n_models, classwise)\n",
    "            # combine logits\n",
    "            if classwise:\n",
    "                # for each class c, weighted sum L[:,:,c] with W[:,c]\n",
    "                logits_comb = np.zeros((L_tr.shape[0], 3), dtype=np.float64)\n",
    "                for c in range(3):\n",
    "                    logits_comb[:, c] = (L_tr[:, :, c] @ W[:, c])\n",
    "            else:\n",
    "                # single weight vector shared for all classes\n",
    "                logits_comb = np.tensordot(L_tr, W, axes=([1],[0]))  # [N,3]\n",
    "            probs = softmax(logits_comb, axis=1)\n",
    "            # log loss\n",
    "            y_onehot = np.eye(3)[y_tr]\n",
    "            loss = -np.mean((y_onehot * np.log(np.clip(probs, 1e-12, 1.0))).sum(axis=1))\n",
    "            return loss\n",
    "\n",
    "        for k, (tr_idx, va_idx) in enumerate(splits):\n",
    "            L_tr, L_va = L[tr_idx], L[va_idx]\n",
    "            y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "            res = minimize(\n",
    "                fold_objective,\n",
    "                x0=theta0,\n",
    "                args=(L_tr, y_tr),\n",
    "                method=\"L-BFGS-B\",\n",
    "                options={\"maxiter\": max_iter, \"ftol\": tol, \"gtol\": tol}\n",
    "            )\n",
    "            theta_star = res.x\n",
    "            W_star = self._param_to_weights(theta_star, n_models, classwise)\n",
    "\n",
    "            # validate\n",
    "            if classwise:\n",
    "                logits_va = np.zeros((len(va_idx), 3), dtype=np.float64)\n",
    "                for c in range(3):\n",
    "                    logits_va[:, c] = (L_va[:, :, c] @ W_star[:, c])\n",
    "            else:\n",
    "                logits_va = np.tensordot(L_va, W_star, axes=([1], [0]))\n",
    "            probs_va = softmax(logits_va, axis=1)\n",
    "            oof[va_idx] = probs_va\n",
    "            fold_loss = log_loss(y_va, probs_va, labels=[0,1,2])\n",
    "            cv_losses.append(fold_loss)\n",
    "            best_thetas.append(theta_star)\n",
    "\n",
    "        mean_cv = float(np.mean(cv_losses))\n",
    "        std_cv  = float(np.std(cv_losses))\n",
    "\n",
    "        # store the average theta across folds (simple mean; could also refit on full)\n",
    "        theta_bar = np.mean(np.vstack(best_thetas), axis=0)\n",
    "        self.weights = self._param_to_weights(theta_bar, n_models, classwise)\n",
    "        self.classwise = classwise\n",
    "\n",
    "        return {\n",
    "            \"cv_losses\": cv_losses,\n",
    "            \"cv_mean\": mean_cv,\n",
    "            \"cv_std\":  std_cv,\n",
    "            \"oof_probs\": oof,\n",
    "            \"weights_matrix\": self.weights.tolist() if classwise else self.weights.tolist(),\n",
    "            \"classwise\": classwise,\n",
    "            \"model_order\": ms,\n",
    "        }\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, model_names_in_feats: List[str], eps: float = 1e-9) -> np.ndarray:\n",
    "        assert self.weights is not None, \"Call fit_cv first.\"\n",
    "        ms = model_names_in_feats\n",
    "        n_models = len(ms)\n",
    "        N = len(X)\n",
    "        P = np.zeros((N, n_models, 3), dtype=np.float64)\n",
    "        for i, m in enumerate(ms):\n",
    "            P[:, i, 0] = X[f\"{m}_pA\"].values\n",
    "            P[:, i, 1] = X[f\"{m}_pB\"].values\n",
    "            P[:, i, 2] = X[f\"{m}_pTie\"].values\n",
    "        L = np.log(np.clip(P, eps, 1 - eps))\n",
    "\n",
    "        if self.classwise:\n",
    "            logits = np.zeros((N, 3), dtype=np.float64)\n",
    "            for c in range(3):\n",
    "                logits[:, c] = (L[:, :, c] @ self.weights[:, c])\n",
    "        else:\n",
    "            logits = np.tensordot(L, self.weights, axes=([1], [0]))\n",
    "        return softmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "587ec62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# BLEND STRATEGY 2: Stacking (multinomial logistic regression)\n",
    "# ---------------------------------\n",
    "@dataclass\n",
    "class StackingLR:\n",
    "    feat_names: List[str]\n",
    "    C: float = 2.0\n",
    "    max_iter: int = 2000\n",
    "    solver: str = \"lbfgs\"\n",
    "    scaler: Optional[StandardScaler] = None\n",
    "    clf: Optional[LogisticRegression] = None\n",
    "\n",
    "    def fit_cv(self, X: pd.DataFrame, y: np.ndarray, splits: List[Tuple[np.ndarray, np.ndarray]]) -> Dict:\n",
    "        if len(self.feat_names) == 0:\n",
    "            raise ValueError(\"StackingLR: No features provided.\")\n",
    "\n",
    "        oof = np.zeros((len(X), 3), dtype=np.float64)\n",
    "        cv_losses = []\n",
    "\n",
    "        for k, (tr_idx, va_idx) in enumerate(splits):\n",
    "            Xtr = X.iloc[tr_idx][self.feat_names].values\n",
    "            Xva = X.iloc[va_idx][self.feat_names].values\n",
    "            ytr = y[tr_idx]\n",
    "            yva = y[va_idx]\n",
    "\n",
    "            pipe = Pipeline([\n",
    "                (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "                (\"clf\", LogisticRegression(\n",
    "                    C=self.C, max_iter=self.max_iter, solver=self.solver,\n",
    "                    multi_class=\"multinomial\"\n",
    "                )),\n",
    "            ])\n",
    "            pipe.fit(Xtr, ytr)\n",
    "            pva = pipe.predict_proba(Xva)\n",
    "            oof[va_idx] = pva\n",
    "            loss = log_loss(yva, pva, labels=[0,1,2])\n",
    "            cv_losses.append(loss)\n",
    "\n",
    "        mean_cv = float(np.mean(cv_losses))\n",
    "        std_cv  = float(np.std(cv_losses))\n",
    "\n",
    "        # Refit on full train\n",
    "        self.clf = LogisticRegression(\n",
    "            C=self.C, max_iter=self.max_iter, solver=self.solver, multi_class=\"multinomial\"\n",
    "        )\n",
    "        self.scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "        Xfull = X[self.feat_names].values\n",
    "        self.scaler.fit(Xfull)\n",
    "        Xs = self.scaler.transform(Xfull)\n",
    "        self.clf.fit(Xs, y)\n",
    "\n",
    "        return {\n",
    "            \"cv_losses\": cv_losses,\n",
    "            \"cv_mean\": mean_cv,\n",
    "            \"cv_std\":  std_cv,\n",
    "            \"oof_probs\": oof,\n",
    "        }\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        assert self.clf is not None and self.scaler is not None, \"Call fit_cv first.\"\n",
    "        Xs = self.scaler.transform(X[self.feat_names].values)\n",
    "        return self.clf.predict_proba(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afebdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# SYMMETRY & CALIBRATION CHECKS\n",
    "# ---------------------------------\n",
    "def symmetry_unit_test(p_df: pd.DataFrame, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    If we swap A<->B probs at inputs for every model, the final predictions\n",
    "    should also swap A<->B (tie unchanged). This function checks that behavior\n",
    "    **for simple weighted averaging sanity**. For stacking, we test by swapping\n",
    "    the per-model features accordingly and verifying predictions swap.\n",
    "    \"\"\"\n",
    "    # This is provided as a placeholder for targeted tests if desired.\n",
    "    # In practice, after producing final test predictions, we will duplicate\n",
    "    # the pipeline, swap A/B in inputs, and verify output swap holds â€œclose enoughâ€.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51a9fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tie_rate_report(oof_probs: np.ndarray, y_true: np.ndarray) -> Dict:\n",
    "    pred_tie = (oof_probs.argmax(axis=1) == 2).mean()\n",
    "    true_tie = (y_true == 2).mean()\n",
    "    return {\"pred_tie_rate\": float(pred_tie), \"true_tie_rate\": float(true_tie)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c20e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# ---------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ---------------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Combiner / Ensembler\")\n",
    "    parser.add_argument(\"--train_csv\", type=str, default=CONFIG[\"train_csv\"])\n",
    "    parser.add_argument(\"--test_csv\",  type=str, default=CONFIG[\"test_csv\"])\n",
    "    parser.add_argument(\"--runs_dir\",  type=str, default=CONFIG[\"runs_dir\"])\n",
    "    parser.add_argument(\"--tag\",       type=str, default=None, help=\"Optional run tag; else timestamped.\")\n",
    "    parser.add_argument(\"--classwise_weights\", action=\"store_true\", help=\"Learn classwise weights for logit blend.\")\n",
    "    args = parser.parse_args() if \"__file__\" in globals() else argparse.Namespace(\n",
    "        train_csv=CONFIG[\"train_csv\"],\n",
    "        test_csv=CONFIG[\"test_csv\"],\n",
    "        runs_dir=CONFIG[\"runs_dir\"],\n",
    "        tag=None,\n",
    "        classwise_weights=False,\n",
    "    )\n",
    "\n",
    "    np.random.seed(CONFIG[\"random_state\"])\n",
    "\n",
    "    # Output dirs\n",
    "    run_tag = args.tag or time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(args.runs_dir, run_tag)\n",
    "    ensure_dir(run_dir)\n",
    "    logging.info(f\"Run dir: {run_dir}\")\n",
    "\n",
    "    # Load official\n",
    "    logging.info(\"Loading official train/test...\")\n",
    "    train_df, test_df = load_official(args.train_csv, args.test_csv, CONFIG[\"use_length_features\"])\n",
    "    y = train_df[\"label\"].values\n",
    "\n",
    "    # Load model predictions\n",
    "    logging.info(\"Loading model artifacts...\")\n",
    "    preds_list: List[ModelPreds] = []\n",
    "    for spec in CONFIG[\"model_specs\"]:\n",
    "        mp = try_load_model_preds(spec)\n",
    "        # Keep only if at least one side (train or test) exists\n",
    "        if (mp.train is not None) or (mp.test is not None):\n",
    "            preds_list.append(mp)\n",
    "        else:\n",
    "            logging.warning(f\"Skipping model '{spec['name']}' (no files found).\")\n",
    "\n",
    "    if len(preds_list) == 0:\n",
    "        raise RuntimeError(\"No model predictions found. Add at least one to CONFIG['model_specs'].\")\n",
    "\n",
    "    # Build features\n",
    "    logging.info(\"Building meta features...\")\n",
    "    Xtr, Xte, feat_names = build_feature_matrices(\n",
    "        train_df=train_df, test_df=test_df,\n",
    "        preds_list=preds_list,\n",
    "        use_engineered=CONFIG[\"stack_use_engineered_features\"],\n",
    "        use_len_feats=CONFIG[\"use_length_features\"],\n",
    "        eps=CONFIG[\"epsilon\"],\n",
    "    )\n",
    "\n",
    "    # Which models are actually available in features?\n",
    "    model_names_available = []\n",
    "    for mp in preds_list:\n",
    "        mname = mp.name\n",
    "        if all(f\"{mname}_{ch}\" in feat_names for ch in (\"pA\",\"pB\",\"pTie\")):\n",
    "            model_names_available.append(mname)\n",
    "    logging.info(f\"Models available for blending: {model_names_available}\")\n",
    "    if len(model_names_available) == 0:\n",
    "        raise RuntimeError(\"No per-model probability features present. Check file paths/columns.\")\n",
    "\n",
    "    # Build CV splits\n",
    "    logging.info(f\"Creating GroupKFold(n={CONFIG['n_folds']}) splits by prompt...\")\n",
    "    splits = cv_splits(train_df, CONFIG[\"n_folds\"])\n",
    "\n",
    "    results_summary = {}\n",
    "    oof_candidates = {}\n",
    "\n",
    "    # Strategy 1: Weighted Logit Blend\n",
    "    if CONFIG[\"use_weighted_logit_blend\"]:\n",
    "        logging.info(\"==> Strategy: Weighted Logit Blend (logit-space)\")\n",
    "        wblend = WeightedLogitBlend(model_names=model_names_available)\n",
    "        wb_info = wblend.fit_cv(\n",
    "            X=Xtr, feat_names=feat_names, y=y, splits=splits,\n",
    "            model_names_in_feats=model_names_available,\n",
    "            max_iter=CONFIG[\"weight_opt_max_iter\"], tol=CONFIG[\"weight_opt_tolerance\"],\n",
    "            init=CONFIG[\"weight_init\"], classwise=args.classwise_weights,\n",
    "        )\n",
    "        results_summary[\"weighted_logit_blend\"] = {\n",
    "            \"cv_mean\": wb_info[\"cv_mean\"],\n",
    "            \"cv_std\": wb_info[\"cv_std\"],\n",
    "            \"cv_losses\": wb_info[\"cv_losses\"],\n",
    "            \"weights\": wb_info[\"weights_matrix\"],\n",
    "            \"classwise\": wb_info[\"classwise\"],\n",
    "            \"model_order\": wb_info[\"model_order\"],\n",
    "        }\n",
    "        oof_candidates[\"weighted_logit_blend\"] = wb_info[\"oof_probs\"]\n",
    "\n",
    "    # Strategy 2: Stacking LR\n",
    "    if CONFIG[\"use_stacking_lr\"]:\n",
    "        logging.info(\"==> Strategy: Stacking (Multinomial Logistic Regression)\")\n",
    "        # Stacking uses ALL feat_names (probs + engineered + lengths) by default\n",
    "        stack = StackingLR(feat_names=feat_names, C=CONFIG[\"stack_C\"],\n",
    "                           max_iter=CONFIG[\"stack_max_iter\"], solver=CONFIG[\"stack_solver\"])\n",
    "        st_info = stack.fit_cv(X=Xtr, y=y, splits=splits)\n",
    "        results_summary[\"stacking_lr\"] = {\n",
    "            \"cv_mean\": st_info[\"cv_mean\"],\n",
    "            \"cv_std\": st_info[\"cv_std\"],\n",
    "            \"cv_losses\": st_info[\"cv_losses\"],\n",
    "        }\n",
    "        oof_candidates[\"stacking_lr\"] = st_info[\"oof_probs\"]\n",
    "\n",
    "    # Pick best strategy by mean CV\n",
    "    if len(results_summary) == 0:\n",
    "        raise RuntimeError(\"No strategies enabled. Enable at least one in CONFIG.\")\n",
    "\n",
    "    best_name = min(results_summary.keys(), key=lambda k: results_summary[k][\"cv_mean\"])\n",
    "    best_cv = results_summary[best_name][\"cv_mean\"]\n",
    "    logging.info(f\"Best strategy: {best_name} (mean CV log-loss={best_cv:.6f})\")\n",
    "\n",
    "    # Produce final predictions on test using the chosen strategy\n",
    "    if best_name == \"weighted_logit_blend\":\n",
    "        final_model = WeightedLogitBlend(model_names=model_names_available)\n",
    "        final_model.weights = np.array(results_summary[\"weighted_logit_blend\"][\"weights\"])\n",
    "        final_model.classwise = results_summary[\"weighted_logit_blend\"][\"classwise\"]\n",
    "        test_probs = final_model.predict(Xte, model_names_in_feats=model_names_available)\n",
    "        oof_final = oof_candidates[\"weighted_logit_blend\"]\n",
    "    else:\n",
    "        # already refit on full train inside StackingLR\n",
    "        test_probs = stack.predict(Xte)\n",
    "        oof_final = oof_candidates[\"stacking_lr\"]\n",
    "\n",
    "    # Diagnostics\n",
    "    tie_stats = tie_rate_report(oof_final, y)\n",
    "    logging.info(f\"Tie rate (true): {tie_stats['true_tie_rate']:.4f} | (pred OOF): {tie_stats['pred_tie_rate']:.4f}\")\n",
    "\n",
    "    # OOF file\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": Xtr[\"id\"].values,\n",
    "        \"pA\": oof_final[:, 0],\n",
    "        \"pB\": oof_final[:, 1],\n",
    "        \"pTie\": oof_final[:, 2],\n",
    "        \"label\": y,\n",
    "    })\n",
    "    oof_path = os.path.join(run_dir, \"oof_combined.csv\")\n",
    "    oof_df.to_csv(oof_path, index=False)\n",
    "    logging.info(f\"Saved OOF combined -> {oof_path}\")\n",
    "\n",
    "    # CV report\n",
    "    cv_report_path = os.path.join(run_dir, \"cv_report.json\")\n",
    "    with open(cv_report_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"strategies\": results_summary,\n",
    "            \"chosen\": best_name,\n",
    "            \"tie_stats\": tie_stats,\n",
    "            \"n_models_used\": len(model_names_available),\n",
    "            \"models_used\": model_names_available,\n",
    "            \"n_folds\": CONFIG[\"n_folds\"],\n",
    "        }, f, indent=2)\n",
    "    logging.info(f\"Saved CV report -> {cv_report_path}\")\n",
    "\n",
    "    # Blend config\n",
    "    blend_cfg_path = os.path.join(run_dir, \"blend_config.json\")\n",
    "    chosen_cfg = {\n",
    "        \"chosen_strategy\": best_name,\n",
    "        \"feat_names\": feat_names,\n",
    "        \"model_names_available\": model_names_available,\n",
    "        \"config\": CONFIG,\n",
    "    }\n",
    "    if best_name == \"weighted_logit_blend\":\n",
    "        chosen_cfg[\"weights\"] = results_summary[\"weighted_logit_blend\"][\"weights\"]\n",
    "        chosen_cfg[\"classwise\"] = results_summary[\"weighted_logit_blend\"][\"classwise\"]\n",
    "    with open(blend_cfg_path, \"w\") as f:\n",
    "        json.dump(chosen_cfg, f, indent=2)\n",
    "    logging.info(f\"Saved blend config -> {blend_cfg_path}\")\n",
    "\n",
    "    # Final Test Submission\n",
    "    sub = pd.DataFrame({\n",
    "        \"id\": Xte[\"id\"].values,\n",
    "        \"winner_model_a\": test_probs[:, 0],\n",
    "        \"winner_model_b\": test_probs[:, 1],\n",
    "        \"winner_tie\":     test_probs[:, 2],\n",
    "    })\n",
    "    # gentle renorm guard\n",
    "    sums = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(axis=1).values\n",
    "    sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values / (sums[:, None] + 1e-12)\n",
    "    sub_path = os.path.join(run_dir, \"submission.csv\")\n",
    "    sub.to_csv(sub_path, index=False)\n",
    "    logging.info(f\"Saved submission -> {sub_path}\")\n",
    "\n",
    "    # Quick symmetry sanity check (best-effort)\n",
    "    try:\n",
    "        # Create an A/B-swapped copy of per-model probs in test features,\n",
    "        # recompute final predictions, and verify swap at the output.\n",
    "        Xte_sw = Xte.copy()\n",
    "        # Swap all *model*_pA <-> *model*_pB features\n",
    "        for m in model_names_available:\n",
    "            a, b = f\"{m}_pA\", f\"{m}_pB\"\n",
    "            if a in Xte_sw.columns and b in Xte_sw.columns:\n",
    "                tmp = Xte_sw[a].copy()\n",
    "                Xte_sw[a] = Xte_sw[b]\n",
    "                Xte_sw[b] = tmp\n",
    "        # The engineered features involving margins were built on original probs,\n",
    "        # so for a rigorous test you'd rebuild engineered features for swapped inputs.\n",
    "        # For a quick check, if using stacking with engineered feats, skip deep check.\n",
    "        if best_name == \"weighted_logit_blend\":\n",
    "            swapped_probs = final_model.predict(Xte_sw, model_names_in_feats=model_names_available)\n",
    "            # Compare to the original: new pA should ~ old pB, new pB ~ old pA, tie ~ tie\n",
    "            deltaA = np.mean(np.abs(swapped_probs[:,0] - test_probs[:,1]))\n",
    "            deltaB = np.mean(np.abs(swapped_probs[:,1] - test_probs[:,0]))\n",
    "            deltaT = np.mean(np.abs(swapped_probs[:,2] - test_probs[:,2]))\n",
    "            logging.info(f\"[Symmetry check] mean |pA_sw - pB|={deltaA:.6f}, |pB_sw - pA|={deltaB:.6f}, |pTie_sw - pTie|={deltaT:.6f}\")\n",
    "        else:\n",
    "            logging.info(\"[Symmetry check] Skipped strict swap test for stacking (engineered feats not rebuilt).\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Symmetry check failed/omitted: {e}\")\n",
    "\n",
    "    logging.info(\"Done.\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ac004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# MAIN PIPELINE\n",
    "# ---------------------------------\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Combiner / Ensembler\")\n",
    "    parser.add_argument(\"--train_csv\", type=str, default=CONFIG[\"train_csv\"])\n",
    "    parser.add_argument(\"--test_csv\",  type=str, default=CONFIG[\"test_csv\"])\n",
    "    parser.add_argument(\"--runs_dir\",  type=str, default=CONFIG[\"runs_dir\"])\n",
    "    parser.add_argument(\"--tag\",       type=str, default=None, help=\"Optional run tag; else timestamped.\")\n",
    "    parser.add_argument(\"--classwise_weights\", action=\"store_true\", help=\"Learn classwise weights for logit blend.\")\n",
    "    args = parser.parse_args() if \"__file__\" in globals() else argparse.Namespace(\n",
    "        train_csv=CONFIG[\"train_csv\"],\n",
    "        test_csv=CONFIG[\"test_csv\"],\n",
    "        runs_dir=CONFIG[\"runs_dir\"],\n",
    "        tag=None,\n",
    "        classwise_weights=False,\n",
    "    )\n",
    "\n",
    "    np.random.seed(CONFIG[\"random_state\"])\n",
    "\n",
    "    # Output dirs\n",
    "    run_tag = args.tag or time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(args.runs_dir, run_tag)\n",
    "    ensure_dir(run_dir)\n",
    "    logging.info(f\"Run dir: {run_dir}\")\n",
    "\n",
    "    # Load official\n",
    "    logging.info(\"Loading official train/test...\")\n",
    "    train_df, test_df = load_official(args.train_csv, args.test_csv, CONFIG[\"use_length_features\"])\n",
    "    y = train_df[\"label\"].values\n",
    "\n",
    "    # Load model predictions\n",
    "    logging.info(\"Loading model artifacts...\")\n",
    "    preds_list: List[ModelPreds] = []\n",
    "    for spec in CONFIG[\"model_specs\"]:\n",
    "        mp = try_load_model_preds(spec)\n",
    "        # Keep only if at least one side (train or test) exists\n",
    "        if (mp.train is not None) or (mp.test is not None):\n",
    "            preds_list.append(mp)\n",
    "        else:\n",
    "            logging.warning(f\"Skipping model '{spec['name']}' (no files found).\")\n",
    "\n",
    "    if len(preds_list) == 0:\n",
    "        raise RuntimeError(\"No model predictions found. Add at least one to CONFIG['model_specs'].\")\n",
    "\n",
    "    # Build features\n",
    "    logging.info(\"Building meta features...\")\n",
    "    Xtr, Xte, feat_names = build_feature_matrices(\n",
    "        train_df=train_df, test_df=test_df,\n",
    "        preds_list=preds_list,\n",
    "        use_engineered=CONFIG[\"stack_use_engineered_features\"],\n",
    "        use_len_feats=CONFIG[\"use_length_features\"],\n",
    "        eps=CONFIG[\"epsilon\"],\n",
    "    )\n",
    "\n",
    "    # Which models are actually available in features?\n",
    "    model_names_available = []\n",
    "    for mp in preds_list:\n",
    "        mname = mp.name\n",
    "        if all(f\"{mname}_{ch}\" in feat_names for ch in (\"pA\",\"pB\",\"pTie\")):\n",
    "            model_names_available.append(mname)\n",
    "    logging.info(f\"Models available for blending: {model_names_available}\")\n",
    "    if len(model_names_available) == 0:\n",
    "        raise RuntimeError(\"No per-model probability features present. Check file paths/columns.\")\n",
    "\n",
    "    # Build CV splits\n",
    "    logging.info(f\"Creating GroupKFold(n={CONFIG['n_folds']}) splits by prompt...\")\n",
    "    splits = cv_splits(train_df, CONFIG[\"n_folds\"])\n",
    "\n",
    "    results_summary = {}\n",
    "    oof_candidates = {}\n",
    "\n",
    "    # Strategy 1: Weighted Logit Blend\n",
    "    if CONFIG[\"use_weighted_logit_blend\"]:\n",
    "        logging.info(\"==> Strategy: Weighted Logit Blend (logit-space)\")\n",
    "        wblend = WeightedLogitBlend(model_names=model_names_available)\n",
    "        wb_info = wblend.fit_cv(\n",
    "            X=Xtr, feat_names=feat_names, y=y, splits=splits,\n",
    "            model_names_in_feats=model_names_available,\n",
    "            max_iter=CONFIG[\"weight_opt_max_iter\"], tol=CONFIG[\"weight_opt_tolerance\"],\n",
    "            init=CONFIG[\"weight_init\"], classwise=args.classwise_weights,\n",
    "        )\n",
    "        results_summary[\"weighted_logit_blend\"] = {\n",
    "            \"cv_mean\": wb_info[\"cv_mean\"],\n",
    "            \"cv_std\": wb_info[\"cv_std\"],\n",
    "            \"cv_losses\": wb_info[\"cv_losses\"],\n",
    "            \"weights\": wb_info[\"weights_matrix\"],\n",
    "            \"classwise\": wb_info[\"classwise\"],\n",
    "            \"model_order\": wb_info[\"model_order\"],\n",
    "        }\n",
    "        oof_candidates[\"weighted_logit_blend\"] = wb_info[\"oof_probs\"]\n",
    "\n",
    "    # Strategy 2: Stacking LR\n",
    "    if CONFIG[\"use_stacking_lr\"]:\n",
    "        logging.info(\"==> Strategy: Stacking (Multinomial Logistic Regression)\")\n",
    "        stack = StackingLR(feat_names=feat_names, C=CONFIG[\"stack_C\"],\n",
    "                           max_iter=CONFIG[\"stack_max_iter\"], solver=CONFIG[\"stack_solver\"])\n",
    "        st_info = stack.fit_cv(X=Xtr, y=y, splits=splits)\n",
    "        results_summary[\"stacking_lr\"] = {\n",
    "            \"cv_mean\": st_info[\"cv_mean\"],\n",
    "            \"cv_std\": st_info[\"cv_std\"],\n",
    "            \"cv_losses\": st_info[\"cv_losses\"],\n",
    "        }\n",
    "        oof_candidates[\"stacking_lr\"] = st_info[\"oof_probs\"]\n",
    "\n",
    "    # Pick best strategy by mean CV\n",
    "    if len(results_summary) == 0:\n",
    "        raise RuntimeError(\"No strategies enabled. Enable at least one in CONFIG.\")\n",
    "\n",
    "    best_name = min(results_summary.keys(), key=lambda k: results_summary[k][\"cv_mean\"])\n",
    "    best_cv = results_summary[best_name][\"cv_mean\"]\n",
    "    logging.info(f\"Best strategy: {best_name} (mean CV log-loss={best_cv:.6f})\")\n",
    "\n",
    "    # Produce final predictions on test using the chosen strategy\n",
    "    if best_name == \"weighted_logit_blend\":\n",
    "        final_model = WeightedLogitBlend(model_names=model_names_available)\n",
    "        final_model.weights = np.array(results_summary[\"weighted_logit_blend\"][\"weights\"])\n",
    "        final_model.classwise = results_summary[\"weighted_logit_blend\"][\"classwise\"]\n",
    "        test_probs = final_model.predict(Xte, model_names_in_feats=model_names_available)\n",
    "        oof_final = oof_candidates[\"weighted_logit_blend\"]\n",
    "    else:\n",
    "        test_probs = stack.predict(Xte)\n",
    "        oof_final = oof_candidates[\"stacking_lr\"]\n",
    "\n",
    "    # Diagnostics\n",
    "    tie_stats = tie_rate_report(oof_final, y)\n",
    "    logging.info(f\"Tie rate (true): {tie_stats['true_tie_rate']:.4f} | (pred OOF): {tie_stats['pred_tie_rate']:.4f}\")\n",
    "\n",
    "    # OOF file\n",
    "    oof_df = pd.DataFrame({\n",
    "        \"id\": Xtr[\"id\"].values,\n",
    "        \"pA\": oof_final[:, 0],\n",
    "        \"pB\": oof_final[:, 1],\n",
    "        \"pTie\": oof_final[:, 2],\n",
    "        \"label\": y,\n",
    "    })\n",
    "    oof_path = os.path.join(run_dir, \"oof_combined.csv\")\n",
    "    oof_df.to_csv(oof_path, index=False)\n",
    "    logging.info(f\"Saved OOF combined -> {oof_path}\")\n",
    "\n",
    "    # CV report\n",
    "    cv_report_path = os.path.join(run_dir, \"cv_report.json\")\n",
    "    with open(cv_report_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"strategies\": results_summary,\n",
    "            \"chosen\": best_name,\n",
    "            \"tie_stats\": tie_stats,\n",
    "            \"n_models_used\": len(model_names_available),\n",
    "            \"models_used\": model_names_available,\n",
    "            \"n_folds\": CONFIG[\"n_folds\"],\n",
    "        }, f, indent=2)\n",
    "    logging.info(f\"Saved CV report -> {cv_report_path}\")\n",
    "\n",
    "    # Blend config\n",
    "    blend_cfg_path = os.path.join(run_dir, \"blend_config.json\")\n",
    "    chosen_cfg = {\n",
    "        \"chosen_strategy\": best_name,\n",
    "        \"feat_names\": feat_names,\n",
    "        \"model_names_available\": model_names_available,\n",
    "        \"config\": CONFIG,\n",
    "    }\n",
    "    if best_name == \"weighted_logit_blend\":\n",
    "        chosen_cfg[\"weights\"] = results_summary[\"weighted_logit_blend\"][\"weights\"]\n",
    "        chosen_cfg[\"classwise\"] = results_summary[\"weighted_logit_blend\"][\"classwise\"]\n",
    "    with open(blend_cfg_path, \"w\") as f:\n",
    "        json.dump(chosen_cfg, f, indent=2)\n",
    "    logging.info(f\"Saved blend config -> {blend_cfg_path}\")\n",
    "\n",
    "    # Final Test Submission\n",
    "    sub = pd.DataFrame({\n",
    "        \"id\": Xte[\"id\"].values,\n",
    "        \"winner_model_a\": test_probs[:, 0],\n",
    "        \"winner_model_b\": test_probs[:, 1],\n",
    "        \"winner_tie\":     test_probs[:, 2],\n",
    "    })\n",
    "    sums = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].sum(axis=1).values\n",
    "    sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]].values / (sums[:, None] + 1e-12)\n",
    "    sub_path = os.path.join(run_dir, \"submission.csv\")\n",
    "    sub.to_csv(sub_path, index=False)\n",
    "    logging.info(f\"Saved submission -> {sub_path}\")\n",
    "\n",
    "    # Quick symmetry sanity check (best-effort)\n",
    "    try:\n",
    "        Xte_sw = Xte.copy()\n",
    "        for m in model_names_available:\n",
    "            a, b = f\"{m}_pA\", f\"{m}_pB\"\n",
    "            if a in Xte_sw.columns and b in Xte_sw.columns:\n",
    "                tmp = Xte_sw[a].copy()\n",
    "                Xte_sw[a] = Xte_sw[b]\n",
    "                Xte_sw[b] = tmp\n",
    "        if best_name == \"weighted_logit_blend\":\n",
    "            swapped_probs = final_model.predict(Xte_sw, model_names_in_feats=model_names_available)\n",
    "            deltaA = np.mean(np.abs(swapped_probs[:,0] - test_probs[:,1]))\n",
    "            deltaB = np.mean(np.abs(swapped_probs[:,1] - test_probs[:,0]))\n",
    "            deltaT = np.mean(np.abs(swapped_probs[:,2] - test_probs[:,2]))\n",
    "            logging.info(f\"[Symmetry check] mean |pA_sw - pB|={deltaA:.6f}, |pB_sw - pA|={deltaB:.6f}, |pTie_sw - pTie|={deltaT:.6f}\")\n",
    "        else:\n",
    "            logging.info(\"[Symmetry check] Skipped strict swap test for stacking (engineered feats not rebuilt).\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Symmetry check failed/omitted: {e}\")\n",
    "\n",
    "    # ---------------------------------\n",
    "    # SUMMARY PRINTOUT (final weights + CV score)\n",
    "    # ---------------------------------\n",
    "    try:\n",
    "        print(\"\\n=== FINAL ENSEMBLE SUMMARY ===\")\n",
    "        if \"weighted_logit_blend\" in results_summary:\n",
    "            weights = results_summary[\"weighted_logit_blend\"][\"weights\"]\n",
    "            model_order = results_summary[\"weighted_logit_blend\"][\"model_order\"]\n",
    "            mean_cv = results_summary[\"weighted_logit_blend\"][\"cv_mean\"]\n",
    "            std_cv  = results_summary[\"weighted_logit_blend\"][\"cv_std\"]\n",
    "\n",
    "            for name, w in zip(model_order, weights):\n",
    "                print(f\"{name}: {w:.2f}\")\n",
    "            print(f\"Mean CV log-loss: {mean_cv:.3f} Â± {std_cv:.3f}\")\n",
    "\n",
    "            summary_path = os.path.join(run_dir, \"ensemble_summary.txt\")\n",
    "            with open(summary_path, \"w\") as f:\n",
    "                f.write(\"=== FINAL ENSEMBLE SUMMARY ===\\n\")\n",
    "                for name, w in zip(model_order, weights):\n",
    "                    f.write(f\"{name}: {w:.2f}\\n\")\n",
    "                f.write(f\"Mean CV log-loss: {mean_cv:.3f} Â± {std_cv:.3f}\\n\")\n",
    "            print(f\"Saved summary -> {summary_path}\")\n",
    "        else:\n",
    "            print(\"No weighted_logit_blend results available.\")\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Could not print ensemble summary: {e}\")\n",
    "\n",
    "    logging.info(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec037b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:18:59] INFO - Run dir: ./runs/20251113_021859\n",
      "[02:18:59] INFO - Loading official train/test...\n",
      "[02:19:01] INFO - Loading model artifacts...\n",
      "[02:19:01] INFO - Building meta features...\n",
      "[02:19:01] INFO - Models available for blending: ['teacher2_reward', 'deberta_head_only']\n",
      "[02:19:01] INFO - Creating GroupKFold(n=5) splits by prompt...\n",
      "[02:19:01] INFO - ==> Strategy: Weighted Logit Blend (logit-space)\n",
      "[02:19:02] INFO - ==> Strategy: Stacking (Multinomial Logistic Regression)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:04] INFO - Best strategy: stacking_lr (mean CV log-loss=1.005790)\n",
      "[02:19:04] INFO - Tie rate (true): 0.3090 | (pred OOF): 0.2435\n",
      "[02:19:04] INFO - Saved OOF combined -> ./runs/20251113_021859/oof_combined.csv\n",
      "[02:19:04] INFO - Saved CV report -> ./runs/20251113_021859/cv_report.json\n",
      "[02:19:04] INFO - Saved blend config -> ./runs/20251113_021859/blend_config.json\n",
      "[02:19:04] INFO - Saved submission -> ./runs/20251113_021859/submission.csv\n",
      "[02:19:04] INFO - [Symmetry check] Skipped strict swap test for stacking (engineered feats not rebuilt).\n",
      "[02:19:04] INFO - Done.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52fe17d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission shape: (3, 4)\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.329364        0.276053    0.394583\n",
      "1   211333        0.292047        0.410935    0.297018\n",
      "2  1233961        0.204762        0.454993    0.340245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the generated submission file\n",
    "df = pd.read_csv(\"runs/20251113_021859/submission.csv\")\n",
    "\n",
    "# Print its shape and a few sample rows\n",
    "print(\"Submission shape:\", df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9631a01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train               : (57477, 9)\n",
      "test                : (3, 4)\n",
      "teacher_train       : (57477, 5)\n",
      "teacher_test        : (3, 4)\n",
      "deberta_train       : (57477, 5)\n",
      "deberta_test        : (3, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# check all key files used in CONFIG\n",
    "files = {\n",
    "    \"train\": \"./data/train.csv\",\n",
    "    \"test\": \"./data/test.csv\",\n",
    "    \"teacher_train\": \"./teacher_logits_train_improved.csv\",\n",
    "    \"teacher_test\": \"./teacher_logits_test_improved.csv\",\n",
    "    \"deberta_train\": \"./deberta_oof_train.csv\",\n",
    "    \"deberta_test\": \"./submission_deberta_small_calibrated.csv\",\n",
    "}\n",
    "\n",
    "for name, path in files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"{name:20s}: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name:20s}: ERROR â†’ {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8bc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
